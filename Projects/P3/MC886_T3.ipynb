{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf6361a3-45da-4505-8e24-2fd53afd4bee",
      "metadata": {
        "id": "cf6361a3-45da-4505-8e24-2fd53afd4bee",
        "tags": []
      },
      "source": [
        "# Group information\n",
        "\n",
        "Names: Andreas Cisi Ramos e João Pedro de Moraes Novaes\n",
        "\n",
        "\n",
        "RAs: 246932 e 174494"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed070eb-2db6-4905-9f49-4a325bfde24d",
      "metadata": {
        "id": "3ed070eb-2db6-4905-9f49-4a325bfde24d",
        "tags": []
      },
      "source": [
        "## Objective:\n",
        "\n",
        "To explore **deep learning** techniques, focused on **Convolutional Neural Networks**. In this task you'll be architecturing different CNNs to solve an image classification problem.\n",
        "\n",
        "This **MUST** be developed using the pytorch and Sklearn libraries (PyTorch Lightning is **not** allowed).\n",
        "\n",
        "*Tip: Use the Pillow (PIL) library to work with images with pytorch. Also, you can use [tqdm](https://github.com/tqdm/tqdm) library to see the progress of the training process.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDX4xG1wKwH9",
      "metadata": {
        "id": "tDX4xG1wKwH9"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset you should use is an adaptation of the \"CIFAR10\" dataset. The CIFAR10 dataset is a dataset of 32x32 images of 10 different classes, and is extensivelly used for classification of images using CNN.\n",
        "\n",
        "In this work, You will only use CIFAR10 data of 3 classes: airplane, bird and cat. You also will only use a limited amount of the data of each class (the original dataset has 6000 images per class). For each class, the amount of images is 400 for training, 200 for validation and 300 for test. This is defined so that the training steps are faster, but it makes the problem harder, as we are working with few data points.\n",
        "\n",
        "\n",
        "The dataset will be available in the [same folder](https://drive.google.com/drive/folders/14uiy_7xMq5LOqODBzbIJLD4Vq0E9XD5v) as the other tasks, in the \"cifar_mod\" folder. You can copy the dataset folder or download it locally.\n",
        "\n",
        "Here are some examples of each class from the dataset:\n",
        "\n",
        "**airplane**\n",
        "\n",
        "**bird**\n",
        "\n",
        "\n",
        "**cat**\n",
        "\n",
        "\n",
        "\n",
        "As in every machine learning task, we need to understand and analyze the data. From the way that the dataset was collected (data collection protocol, equipment used, people involved), to its final result (resulting files) and objective. When dealing with images, we usually want to track possible biases that different classes may have when collected. This dataset contains images of the target objects in a centered position in the foreground, having few biases related to the common background of some classes (for example: airplanes in the blue sky).\n",
        "\n",
        "As in the other tasks we already covered the data analysis part, we will focus on the technical machine learning parts for this one. Just remember that images are another type of data and we also could (and **should**) analyze the information before applying machine learning techniques blindly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e9ef69-28e6-444f-8a04-e03f1ff34fbc",
      "metadata": {
        "id": "77e9ef69-28e6-444f-8a04-e03f1ff34fbc"
      },
      "source": [
        "## Load the dataset (2 Points)\n",
        "\n",
        "When working with Pytorch, we need to create a \"Dataset\" class that usually will handle the data loading, as well as the data transformations, and will allow us to retrieve the data with the respective label.\n",
        "\n",
        "Data transformations are an important part of Pytorch datasets, as we already studied in the lectures that data augmentation can be powerful in training deep neural networks. But some of those transformations are just to define the entry of the model as Pytorch tensors or normalizing data, which can be impactful as well.\n",
        "\n",
        "Create a Pytorch Dataset and Dataloader with and without data augmentation for training data (to compare later). The `Dataset` class has 3 required methods: `__init__`, `__len__`, and `__getitem__`. In the code below we have some basic idea of an ImageDataset.\n",
        "\n",
        "You can choose how to implement the image loading in the class:\n",
        "- Inside the `__getitem__` method (low memory usage for each dataset instance, slower for training)\n",
        "- Inside the `__init__` method (saves all images on memory, but training is faster)\n",
        "\n",
        "Either implementation is ok, if the machine you're using supports the loading of all images this is probably better for performance.\n",
        "\n",
        "For a deeper look at Pytorch datasets and dataloaders, look [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "The images are already divided by folder for train, validation and test, and you should follow this order. The targets of each image are described in the filename in the following pattern: `[image_number]_[class].jpg`.\n",
        "\n",
        "To create the data augmentation you can follow the `preparation` example below, and add different Pytorch transformations. A list of available transformations (as well as examples) can be found in the [official documentation (here)](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "IbEvl-1GHkNT",
      "metadata": {
        "id": "IbEvl-1GHkNT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# IMPORTS\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OZC2Q6qmvi13",
      "metadata": {
        "id": "OZC2Q6qmvi13"
      },
      "outputs": [],
      "source": [
        "input_width =  32 \n",
        "input_height = 32\n",
        "nchannels = 3\n",
        "\n",
        "# train transformations - Data Augmentation : transformaçoes como flip / rotação / brilho \n",
        "augmentation = transforms.Compose([\n",
        "    transforms.Resize((input_width, input_height), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# test transformations - here we should only have the transformations needed for preparing the data for the model\n",
        "preparation = transforms.Compose([\n",
        "              # make all images the same size with a specific interpolation method\n",
        "              transforms.Resize((input_width,input_height), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                                max_size=None, antialias=True),\n",
        "              # transform the image to Tensor (this will change the configuration from Height x Width x Channels to Channels x Height x Width)\n",
        "              transforms.ToTensor(),\n",
        "              # this normalization is just an example, based on the ImageNet mean and standard deviation of images. You can test without it if you want!\n",
        "              transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "          ])\n",
        "\n",
        "class ImageDataset():\n",
        "  def __init__(self, directoryRoot, transform=None):\n",
        "\n",
        "    #self variables\n",
        "    self.directoryRoot = directoryRoot\n",
        "    self.transform = transform\n",
        "    self.filenames = []\n",
        "    self.indexToClassDictionary = {0:'airplane', 1:'bird', 2:'cat'}\n",
        "    \n",
        "    for root, _, files in os.walk(directoryRoot):\n",
        "      for file in files:\n",
        "          self.filenames.append(os.path.join(root, file))\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.filenames) # Attention: change this line if your __init__ method uses another attribute to keep the images/filenames\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    Return a tuple with the image (as a Tensor) and the respective target at position idx.\n",
        "    '''\n",
        "    imageName = self.filenames[idx]\n",
        "    image = Image.open(imageName).convert('RGB') \n",
        " \n",
        "    # For debbuging\n",
        "    #print(os.path.basename(imageName)) \n",
        "\n",
        "    # Retirando o index do nome do arquivo\n",
        "    labelIndex = int( os.path.basename(imageName).split('_')[-1].split('.')[0] )\n",
        "    label = self.indexToClassDictionary[ labelIndex ]  \n",
        "    \n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    \n",
        "    return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51d5fc3",
      "metadata": {},
      "source": [
        "### Criando os Datasets e os Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "_x1yZOzLAjJG",
      "metadata": {
        "id": "_x1yZOzLAjJG"
      },
      "outputs": [],
      "source": [
        "# Criando os Datasets\n",
        "trainDataset = ImageDataset('cifar_mod/train', augmentation)\n",
        "validationDataset = ImageDataset( 'cifar_mod/valid', preparation)\n",
        "testDataset = ImageDataset('cifar_mod/test', preparation)\n",
        "\n",
        "#Criando os DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "trainLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True, num_workers=2,persistent_workers=True)\n",
        "validationLoader = DataLoader(validationDataset, batch_size=batch_size, shuffle=False, num_workers=2,persistent_workers=True)\n",
        "testLoader = DataLoader(testDataset, batch_size=batch_size, shuffle=False, num_workers=2,persistent_workers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c0f348",
      "metadata": {},
      "source": [
        "## Testando o metodo __get_item__ e a visualização de imagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ed36db73-2950-4360-8b9f-6de062bf3991",
      "metadata": {
        "id": "ed36db73-2950-4360-8b9f-6de062bf3991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 1200\n",
            "Images are 32x32x3\n",
            "Class of the image:  bird\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize images from the training set\n",
        "\n",
        "datatensor = trainDataset\n",
        "print(\"Number of images:\", len(datatensor))\n",
        "image, target = datatensor[100] # This will execute the '__getitem__' method\n",
        "nchannels = image.shape[0]\n",
        "height    = image.shape[1]\n",
        "width     = image.shape[2]\n",
        "image     = image.permute(1,2,0).numpy() # Converts the Tensor back to image shape\n",
        "image     = 255*(image - np.min(image))/(np.max(image)-np.min(image))\n",
        "image     = image.astype('uint8')\n",
        "print(\"Images are {}x{}x{}\".format(width,height,nchannels))\n",
        "plt.imshow(image)\n",
        "print(\"Class of the image: \", target)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6rzFdKIWOvt",
      "metadata": {
        "id": "H6rzFdKIWOvt"
      },
      "source": [
        "## Train a baseline model (1 Point)\n",
        "\n",
        "Before going for the deep learning approach, you should test a baseline model in this problem. To do so, train a RandomForectClassifier, where the inputs are the flattened images (all pixes of 3 channels concatenated).\n",
        "\n",
        "You should use the same train/validation/test division that you'll be using in the next section. You can use the Sklearn library for this task. Remember to test and plot a confusion matrix with the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8pp9lA-2WS4-",
      "metadata": {
        "id": "8pp9lA-2WS4-"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87632bba",
      "metadata": {},
      "source": [
        "### Load Data\n",
        "\n",
        "Para carregar os dados é necessário carregar as imagens e achatar cada uma delas para criar um vetor (1D). Isso se deve ao fato de cada imagem ser 32x32 pixels e ter 3 canais de cor . Com isso, é necessário converter para um vetor unidimensional de 3072 elementos (32 * 32 * 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f49c0b8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadDataset(dataset):\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    for i in range(len(dataset)):\n",
        "        image, label = dataset[i]\n",
        "        # Achatamento\n",
        "        image = image.view(-1).numpy()\n",
        "\n",
        "        data.append(image)\n",
        "        labels.append(label)\n",
        "    \n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e96b1e",
      "metadata": {},
      "source": [
        "### Carregar dados de treinamento, validação e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "66608e18",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train = loadDataset(trainDataset)\n",
        "X_validation, y_validation = loadDataset(validationDataset)\n",
        "X_test, y_test = loadDataset(testDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "316dcad0",
      "metadata": {},
      "source": [
        "#### Treinar o RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0838f564",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "randomForest = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "randomForest.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74930a4b",
      "metadata": {},
      "source": [
        "#### Acurácia no conjunto de validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3dcfb482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 67.83%\n"
          ]
        }
      ],
      "source": [
        "y_validation_pred = randomForest.predict(X_validation)\n",
        "\n",
        "validation_accuracy = np.mean(y_validation_pred == y_validation)\n",
        "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ceb03b",
      "metadata": {},
      "source": [
        "#### Acurácia no conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d071c7ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 63.33%\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = randomForest.predict(X_test)\n",
        "\n",
        "test_accuracy = np.mean(y_test_pred == y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be99ba5",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d868b732",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGwCAYAAABo5yU1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKJElEQVR4nO3deVxU9foH8M8MyLDNDIKsOqCIIu5bKdeVNNfUknLJEhOXVFIhE72GgWl4tXJPS0vUq6GVUmraz31JtFBRUyQhFFRQcwFBWef8/iDmNgHKMAMzx/m87+u8Lmd/RlIenuf7PUciCIIAIiIiIhMlNXYARERERE/CZIWIiIhMGpMVIiIiMmlMVoiIiMikMVkhIiIik8ZkhYiIiEwakxUiIiIyaZbGDsCcqdVq3Lx5E3K5HBKJxNjhEBGRjgRBwMOHD+Hh4QGptOZ+/8/Pz0dhYaHe17GysoK1tbUBIqpdTFaM6ObNm1CpVMYOg4iI9JSRkYEGDRrUyLXz8/NhI3cCih/pfS03NzekpaWJLmFhsmJEcrkcAGDVPAgSCysjR0M1Lf3wx8YOgYgM7GFODnwaqTT/nteEwsJCoPgRZM2DAH1+VpQUIuvSBhQWFjJZoaora/1ILKyYrJgBhUJh7BCIqIbUSivf0lqvnxWCRLzDVJmsEBERiYEEgD5JkYiHRjJZISIiEgOJtHTR53yREm/kREREZBZYWSEiIhIDiUTPNpB4+0BMVoiIiMSAbSAiIiIi08TKChERkRiwDURERESmTc82kIibKeKNnIiIiMwCKytERERiwDYQERERmTTOBiIiIiIyTaysEBERiQHbQERERGTSzLgNxGSFiIhIDMy4siLeNIuIiIjMAisrREREYsA2EBEREZk0iUTPZIVtICIiIqIawcoKERGRGEglpYs+54sUkxUiIiIxMOMxK+KNnIiIiMwCKytERERiYMbPWWGyQkREJAZsAxERERGZJlZWiIiIxIBtICIiIjJpZtwGYrJCREQkBmZcWRFvmkVERERmgZUVIiIiMWAbiIiIiEwa20BEREREpomVFSIiIlHQsw0k4voEkxUiIiIxYBuIiIiIyDSxskJERCQGEomes4HEW1lhskJERCQGZjx1WbyRExERkVlgZYWIiEgMzHiALZMVIiIiMTDjNhCTFSIiIjEw48qKeNMsIiIiqjHR0dF47rnnIJfL4eLigpdffhnJyclax+Tn52PKlClwcnKCvb09AgMDcevWLa1j0tPTMXDgQNja2sLFxQXvvfceiouLdYqFyQoREZEYlLWB9Fl0cOTIEUyZMgUnT57Evn37UFRUhD59+iAvL09zTGhoKHbu3IlvvvkGR44cwc2bNzF06FDN/pKSEgwcOBCFhYU4ceIENmzYgJiYGMydO1e3jy4IgqDTGWQwOTk5UCqVkLUaD4mFlbHDoRp2/9eVxg6BiAwsJycHrk5KZGdnQ6FQ1Ng9lEolZC+tgKSOTbWvIxQ9RsGud6od6507d+Di4oIjR46ge/fuyM7OhrOzM7Zs2YJXX30VAHD58mX4+fkhPj4enTt3xp49e/DSSy/h5s2bcHV1BQCsWbMG4eHhuHPnDqysqvazj5UVIiIiM5KTk6O1FBQUVOm87OxsAICjoyMA4PTp0ygqKkLv3r01xzRr1gyenp6Ij48HAMTHx6NVq1aaRAUA+vbti5ycHFy8eLHKMTNZISIiEgGJRKL3AgAqlQpKpVKzREdHP/XearUa06dPR5cuXdCyZUsAQFZWFqysrODg4KB1rKurK7KysjTH/D1RKdtftq+qOBuIiIhIBP6ecFTzAgCAjIwMrTaQTCZ76qlTpkzBb7/9huPHj1f//npgZYWIiMiMKBQKreVpyUpISAh27dqFQ4cOoUGDBprtbm5uKCwsxIMHD7SOv3XrFtzc3DTH/HN2UNl62TFVwWSFiIhIDCQGWHQgCAJCQkKwY8cOHDx4EI0aNdLa36FDB9SpUwcHDhzQbEtOTkZ6ejr8/f0BAP7+/rhw4QJu376tOWbfvn1QKBRo3rx5lWNhG4iIiEgEDNUGqqopU6Zgy5Yt+P777yGXyzVjTJRKJWxsbKBUKhEcHIywsDA4OjpCoVDgnXfegb+/Pzp37gwA6NOnD5o3b44333wTixYtQlZWFt5//31MmTKlSu2nMkxWiIiIqJzVq1cDAHr27Km1ff369RgzZgwAYMmSJZBKpQgMDERBQQH69u2Lzz77THOshYUFdu3ahUmTJsHf3x92dnYICgrCvHnzdIqFyQoREZEI1HZlpSqPYbO2tsaqVauwatWqSo/x8vLCjz/+qNO9/4nJChERkQjUdrJiSpisEBERiQCTFRN09epVNGrUCGfPnkXbtm31utaYMWPw4MEDxMXFGSQ2Ki90TB+8FNAGTbxckV9QhF/O/4HIld8j5VrpCHAHhS1mTxiIgM7N0MC1Lu4+yMXuw+fx0ZpdyMnLL3e9uko7HNs8C/Vd68Ir4D3k5D6u7Y9EVfTlt8fw1XfHkJF5DwDQzNsN7wX3x4tdWgAAbv2Zg7nLd+DwqcvIfVQAHy8XvDu2Lwa/0M6YYVM18HtNxmKyyYpKpUJmZibq1atn7FCoCv7V3gfrvjmKs5euwdLCAhGTB2H7ihB0HjYfj/IL4e6shJuzEnOX7cDlP7KgcnfEp7NGwM1ZiTGzvix3vRXvv45LKTdR37WuET4N6cLDxQEfhAxBY5UzBEHA17tPYdSML3Dkv7Pg19gdkyI3IvvhY2z5dCKclPb49qcEvDX7KxzaOBOtfVXGDp90wO+1kVVj+nG580XKZJ+zYmFhATc3N1haVpxPCYKg8yumqea8NvUzfL3rFC7/kYXfrtzA5Kj/QuXuiLZ+pf9AJaVmIih8HfYe+w1Xb/yJYwm/Y/7qnejXrSUsLLT/Mxwb2BVKuS1W/PdARbciE9O/eyv06dICjT1d4OPliojJg2FnK0PCb2kAgF/O/4Hxw3ugQ4uGaNigHmYE94NSboPEpAwjR0664vfauAz1uH0xMmqysnfvXnTt2hUODg5wcnLCSy+9hNTUVAClbSCJRILExEQAwOHDhyGRSLBnzx506NABMpkMx48fR2RkJNq2bYvPP/8cKpUKtra2GDZsmOaFS7re9+/33r59OwICAmBra4s2bdpoXsxU5vjx4+jWrRtsbGygUqkwdepUrVdnmzOFvTUA4H7Ooyce8zAvHyUlas0230ZueG9cf0z6YCPUar4QXGxKStT47v8S8OhxIZ5rVfoAqedbe2PHvtO4n50Htbp0f0FBMbp2aGLkaEkf/F5TbTJqspKXl4ewsDAkJCTgwIEDkEqleOWVV6BWqys9Z9asWVi4cCGSkpLQunVrAEBKSgq2bduGnTt3Yu/evTh79iwmT56s933nzJmDGTNmIDExEU2bNsXIkSM11ZzU1FT069cPgYGBOH/+PLZu3Yrjx48jJCSk0vsWFBSUe9vls0gikSA67FWcTExFUmpmhcc4Ku3wXnB/bNhxQrPNqo4l1s0fgw+Wx+H6rfu1FS4ZwMWUG2jQPQyuXaYjLHorNi0ej2be7gCA9dFjUVxcAu/e4XD913SEfhSLTYvHw1vlbOSoqTr4vTYeiUTf6oqxP0H1GXXMSmBgoNb6V199BWdnZ1y6dAn29vYVnjNv3jy8+OKLWtvy8/OxceNG1K9fHwCwYsUKDBw4EJ988kmF7x540n3L3iYJADNmzMDAgQMBAFFRUWjRogVSUlLQrFkzREdHY9SoUZg+fToAoEmTJli+fDl69OiB1atXw9rautx9o6OjERUV9ZQ/FfH7eOYw+DV2R//xSyrcL7ezxtalk5CclomFX+zWbJ87ZTB+v3oL2/b8WluhkoE08XLF0c2zkZP7GN8fOIvJkZuw6/NpaObtjgVrdiH74WPErXoHjg52+PHIebw1+yv8uHY6WvjUN3bopCN+r41HAn1bOeLNVoxaWbly5QpGjhwJb29vKBQKNGzYEACQnp5e6TkdO3Yst83T01OTqACl7yJQq9VITk7W675llRsAcHcv/c2h7P0G586dQ0xMDOzt7TVL3759oVarkZaWVuF9Z8+ejezsbM2SkfHs9XEXvfca+nZriUGTluPm7Qfl9tvbyvDt8snIfZSPN95bi+K/tYC6P9cUQ3q1w534ZbgTvwzff/YOACB130LMmjCgtj4CVYNVHUt4q5zR1s8TH4QMQcsm9bEm9jDSrt/B2m1HsSLiDfR43hetmjZA+PgBaOfniXXfHDV22FQN/F6TMRi1sjJo0CB4eXlh7dq18PDwgFqtRsuWLVFYWFjpOXZ2drV23zp16mi+Lstmy1pFubm5mDhxIqZOnVru+p6enhXeVyaT6fQuBLFZ9N5rGNizDQa9vQzpN++W2y+3s8a3y6egsKgYr4d9joJC7QHSo2eug431//7M2zX3wqq5b2DAhKVIu36nxuMnw1ELAgoLi/Eov/TvlFSq/RudhYUEAsckPRP4va49fM6KEdy9exfJyclYu3YtunXrBqB0wGp1pKen4+bNm/Dw8AAAnDx5ElKpFL6+vjV23/bt2+PSpUvw8fGpVszPmo/Dh+HVvh3x+owvkPsoHy5OcgBATm4+8guKILezxncrpsDW2goT526A3N4a8r8G4f55PxdqtYCrN/7UuqajsrQVmJyWxeesmLCold+j979aQOVWFw8f5ePbvQk4fvoKvlsxGU0busFb5YzQ6K/x4bRX4Ki0w+7D53HoVDJil7xt7NBJR/xeG5kZT102WrJSt25dODk54YsvvoC7uzvS09Mxa9asal3L2toaQUFB+Pjjj5GTk4OpU6di2LBhFY5XMdR9w8PD0blzZ4SEhGDcuHGws7PDpUuXsG/fPqxcubJan0PMgl/tDgDY/fl0re2Tozbh612n0NpXpZkxcDYuUuuY1oPnah4yReLz5/1cTIrciFt/5kBhb40WPvXx3YrJCOjkBwDYtnQSolZ+j5FhnyPvUQEaqZzxWeSb6PPXg8RIPPi9JmMxWrIilUoRGxuLqVOnomXLlvD19cXy5cvLvd2xKnx8fDB06FAMGDAA9+7dw0svvaT11seauG/r1q1x5MgRzJkzB926dYMgCGjcuDGGDx+uc/zPgrrPVT4LCgB+PnPlqccY4hyqfSsiRj1xf2NPF2xcNL6WoqGaxO+1kenZBhJE3AaSCFV5raIJi4yMRFxcnOZ5LGKSk5MDpVIJWavxkFhYGTscqmH3fzW/ihvRsy4nJweuTkpkZ2dDoVDU2D2USiUcX/8KUivbal9HXfgI97aMrdFYa4rJPm6fiIiI/kffAbZ8gi0RERFRDRF9shIZGSnKFhAREZFOJAZYRIptICIiIhFgG4iIiIjIRLGyQkREJALmXFlhskJERCQC5pyssA1EREREJo2VFSIiIhEw58oKkxUiIiIxMOMXGbINRERERCaNlRUiIiIRYBuIiIiITBqTFSIiIjJp5pyscMwKERERmTRWVoiIiMTAjGcDMVkhIiISAbaBiIiIiEwUKytEREQiYM6VFSYrREREIiCBnsmKiAetsA1EREREJo2VFSIiIhFgG4iIiIhMmxlPXWYbiIiIiEwaKytEREQiYM5tIFZWiIiIRKAsWdFn0cXRo0cxaNAgeHh4QCKRIC4urkrxLF68WHNMw4YNy+1fuHChzp+dlRUiIiIRkEhKF33O10VeXh7atGmDsWPHYujQoeX2Z2Zmaq3v2bMHwcHBCAwM1No+b948jB8/XrMul8t1CwRMVoiIiMxKTk6O1rpMJoNMJit3XP/+/dG/f/9Kr+Pm5qa1/v333yMgIADe3t5a2+VyebljdcU2EBERkQiUVlb0aQOVXkelUkGpVGqW6OhovWO7desWdu/ejeDg4HL7Fi5cCCcnJ7Rr1w6LFy9GcXGxztdnZYWIiEgM9GwDlU1dzsjIgEKh0GyuqKqiqw0bNkAul5drF02dOhXt27eHo6MjTpw4gdmzZyMzMxOffvqpTtdnskJERGRGFAqFVrJiCF999RVGjRoFa2trre1hYWGar1u3bg0rKytMnDgR0dHROiVJbAMRERGJQG3PBqqqY8eOITk5GePGjXvqsZ06dUJxcTGuXr2q0z1YWSEiIhKB2p4NVFVffvklOnTogDZt2jz12MTEREilUri4uOh0DyYrREREVE5ubi5SUlI062lpaUhMTISjoyM8PT0BlM4s+uabb/DJJ5+UOz8+Ph6nTp1CQEAA5HI54uPjERoaijfeeAN169bVKRYmK0RERCIglUoglVa/PCLoeG5CQgICAgI062XjT4KCghATEwMAiI2NhSAIGDlyZLnzZTIZYmNjERkZiYKCAjRq1AihoaFa41iqiskKERGRCNR2G6hnz54QBOGJx0yYMAETJkyocF/79u1x8uRJ3W5aCQ6wJSIiIpPGygoREZEImPOLDJmsEBERiYCpzgaqDUxWiIiIRMCcKyscs0JEREQmjZUVIiIiETDnygqTFSIiIhEw5zErbAMRERGRSWNlhYiISAQk0LMNBPGWVpisEBERiQDbQEREREQmipUVIiIiEeBsICIiIjJpbAMRERERmShWVoiIiESAbSAiIiIyaebcBmKyQkREJALmXFnhmBUiIiIyaaysmICTOz6EXK4wdhhUwwaujjd2CFSLPhrgZ+wQqBbkPcypvZvp2QYS8QNsmawQERGJAdtARERERCaKlRUiIiIR4GwgIiIiMmlsAxERERGZKFZWiIiIRIBtICIiIjJpbAMRERERmShWVoiIiETAnCsrTFaIiIhEgGNWiIiIyKSZc2WFY1aIiIjIpLGyQkREJAJsAxEREZFJYxuIiIiIyESxskJERCQCEujZBjJYJLWPyQoREZEISCUSSPXIVvQ519jYBiIiIiKTxsoKERGRCHA2EBEREZk0zgYiIiIikyaV6L/o4ujRoxg0aBA8PDwgkUgQFxentX/MmDGaBKps6devn9Yx9+7dw6hRo6BQKODg4IDg4GDk5ubq/tl1PoOIiIieeXl5eWjTpg1WrVpV6TH9+vVDZmamZvn666+19o8aNQoXL17Evn37sGvXLhw9ehQTJkzQORa2gYiIiMRAomcr569Tc3JytDbLZDLIZLJyh/fv3x/9+/d/4iVlMhnc3Nwq3JeUlIS9e/fi119/RceOHQEAK1aswIABA/Dxxx/Dw8OjyqGzskJERCQCZQNs9VkAQKVSQalUapbo6Ohqx3T48GG4uLjA19cXkyZNwt27dzX74uPj4eDgoElUAKB3796QSqU4deqUTvdhZYWIiMiMZGRkQKFQaNYrqqpURb9+/TB06FA0atQIqamp+Pe//43+/fsjPj4eFhYWyMrKgouLi9Y5lpaWcHR0RFZWlk73YrJCREQkApK//qfP+QCgUCi0kpXqGjFihObrVq1aoXXr1mjcuDEOHz6MXr166X39v2MbiIiISARqezaQrry9vVGvXj2kpKQAANzc3HD79m2tY4qLi3Hv3r1Kx7lUhskKERER6e369eu4e/cu3N3dAQD+/v548OABTp8+rTnm4MGDUKvV6NSpk07XZhuIiIhIBGr7oXC5ubmaKgkApKWlITExEY6OjnB0dERUVBQCAwPh5uaG1NRUzJw5Ez4+Pujbty8AwM/PD/369cP48eOxZs0aFBUVISQkBCNGjNBpJhDAygoREZEoGGo2UFUlJCSgXbt2aNeuHQAgLCwM7dq1w9y5c2FhYYHz589j8ODBaNq0KYKDg9GhQwccO3ZMa8Du5s2b0axZM/Tq1QsDBgxA165d8cUXX+j82atUWfnhhx+qfMHBgwfrHAQRERGZlp49e0IQhEr3//TTT0+9hqOjI7Zs2aJ3LFVKVl5++eUqXUwikaCkpESfeIiIiKgCUokEUj3aQPqca2xVSlbUanVNx0FERERPwLcuV1N+fj6sra0NFQsRERFVgm9d1kFJSQk+/PBD1K9fH/b29vjjjz8AABEREfjyyy8NHiARERGZN52TlQULFiAmJgaLFi2ClZWVZnvLli2xbt06gwZHREREpWp7NpAp0TlZ2bhxI7744guMGjUKFhYWmu1t2rTB5cuXDRocERERlSobYKvPIlY6Jys3btyAj49Pue1qtRpFRUUGCYqIiIiojM7JSvPmzXHs2LFy27/99lvNg2OIiIjIsCQGWMRK59lAc+fORVBQEG7cuAG1Wo3t27cjOTkZGzduxK5du2oiRiIiIrPH2UA6GDJkCHbu3In9+/fDzs4Oc+fORVJSEnbu3IkXX3yxJmIkIiIiM1at56x069YN+/btM3QsREREVAmppHTR53yxqvZD4RISEpCUlASgdBxLhw4dDBYUERERaTPnNpDOycr169cxcuRI/Pzzz3BwcAAAPHjwAP/6178QGxuLBg0aGDpGIiIiMmM6j1kZN24cioqKkJSUhHv37uHevXtISkqCWq3GuHHjaiJGIiIignk+EA6oRmXlyJEjOHHiBHx9fTXbfH19sWLFCnTr1s2gwREREVEptoF0oFKpKnz4W0lJCTw8PAwSFBEREWkz5wG2OreBFi9ejHfeeQcJCQmabQkJCZg2bRo+/vhjgwZHREREVKXKSt26dbXKR3l5eejUqRMsLUtPLy4uhqWlJcaOHYuXX365RgIlIiIyZ2wDPcXSpUtrOAwiIiJ6En0fmS/eVKWKyUpQUFBNx0FERERUoWo/FA4A8vPzUVhYqLVNoVDoFRARERGVJ5VIINWjlaPPucam8wDbvLw8hISEwMXFBXZ2dqhbt67WQkRERIanzzNWxP6sFZ2TlZkzZ+LgwYNYvXo1ZDIZ1q1bh6ioKHh4eGDjxo01ESMRERGZMZ3bQDt37sTGjRvRs2dPvPXWW+jWrRt8fHzg5eWFzZs3Y9SoUTURJxERkVkz59lAOldW7t27B29vbwCl41Pu3bsHAOjatSuOHj1q2OiIiIgIgHm3gXSurHh7eyMtLQ2enp5o1qwZtm3bhueffx47d+7UvNjQ1PXs2RNt27atdEp2w4YNMX36dEyfPl2n60ZGRiIuLg6JiYl6x/gsuPVnNj79cjeO/5qM/IJCeHrUw4fvDkPLpioAwKpN/4e9hxORdecB6tSxRHOf+pj6Vn+0buZp5MjpSVq4yxHY1gM+zvZwsrPCh3su4+TV+5r9oQGN0buZi9Y5p9MfYO7upHLXspRKsCSwFbzr2eGdbefwx91HNR4/VV9JiRox2w5i37FE3HuQi3p15ejXsz3efLWn5rf29VsP4ODPF3DnbjYsLS3Q1NsD40a+iOZ//b0nqg6dk5W33noL586dQ48ePTBr1iwMGjQIK1euRFFRET799NOaiLHW/frrr7CzszN2GKKW/fAR3gxbhedbN8aa+cGo62CPazfuQGFvozmmYX1n/HvKy2jg7oSCgiJs3HEME2avxY/rw+HoYG/E6OlJrOtYIO3uI+y7fAfv9/Ot8JiE9PtYejBVs15Uoq7wuLH+XribVwjvevz7JgZfxx3F9//3C2aHBKKhygXJqTfwn1XbYWdrjcCB/gAAlUc9TBv3EjxcHVFQWIRvdp3Ae/NjsHlFGByU/D7rw5xnA+mcrISGhmq+7t27Ny5fvozTp0/Dx8cHrVu3NmhwxuLs7PzE/UVFRahTp04tRSNOX207DLd6Dpg/Y7hmWwM3R61jBr7QTmt95oRB2L73F/yelonO7ZrUSpyku9PpD3A6/cETjykqEXD/cfl3iP1dB08HtFcpseCn3/GcF2cSisFvyRno+lwz+HcoTVLdXeri4PHzSEq5rjmmd7c2WudMCeqPHw+cRuq1LHRo3bhW433W6NvKEXGuovuYlX/y8vLC0KFDRZeoFBcXIyQkBEqlEvXq1UNERAQEQQBQ2gb6e4tIIpFg9erVGDx4MOzs7LBgwQIAwMKFC+Hq6gq5XI7g4GDk5+cb46OYpEMnL6JF0wYIm78J3YdF4tXJS/Dtj6cqPb6oqBjf/HgScjtr+HrzhZhi18pDgc1jOuLzkW0xuXsjyGXavxc52NTB1B7e+PhACgqKK666kOlp6avC6Qt/IOPmnwCAlKuZuHD5GjpV8stFUVExdu5LgJ2tNRo3dKvNUJ9JZQNs9VnEqkqVleXLl1f5glOnTq12MLVpw4YNCA4Oxi+//IKEhARMmDABnp6eGD9+fIXHR0ZGYuHChVi6dCksLS2xbds2REZGYtWqVejatSs2bdqE5cuXawYfV6SgoAAFBQWa9ZycHIN/LlNxPfMetu6Kx+ih3TF+xAv47fcMRK+OQ506FhjyYkfNcYdPXsJ70ZuRX1AEZ0c5voiegLosFYva6YwHOJF2D1k5BXBXyBDUyRNRA/0wY8cFqEt/H0DoC43x48VbSLmTBxe5zLgBU5W9/kp35D0uwOhpyyCVSqBWCxg3sjde7N5W67gTCZcxb+k2FBQUwamuPT6ZOwYOCv69puqrUrKyZMmSKl1MIpGIJllRqVRYsmQJJBIJfH19ceHCBSxZsqTSZOX111/HW2+9pVkfMWIEgoODERwcDACYP38+9u/f/8TqSnR0NKKiogz7QUyUWhDQokkDTB/bHwDg51MfV65mYdvueK1k5fm2Pvjus1Dcz8nDt3tOYcaCTdiyfCqcOGZFtI6m3NV8fe3eI1y9+whfvtEerTwUOHcjB4NaucGmjgW+OXvDiFFSdRw68Rv2HzuH96e9hkYqF6RczcTK9T/CybF0oG2Zdi29sW7xFGQ/fITd+39F5KexWB39Nuoq+fdaH1Lo1w7Ru5ViRFWKPS0trUrLH3/8UdPxGkznzp21SmL+/v64cuUKSkpKKjy+Y8eOWutJSUno1KmT1jZ/f/8n3nP27NnIzs7WLBkZGdWM3vQ5O8rR2MtVa5u3ygWZtx9obbO1toJn/Xpo4+eFD8OGwcLCAtv3/lKLkVJNy3pYgOzHRXBXWgMA2tRXopmrHHETOuOHiZ2x7vXSsUtLX22N0Bc4psGUrdm0F6+/3B29uraGt5cb+vRoh1df+hc2b9d+bIWNtRUauDuhRVMVZk4eCgupBX48cNpIUT872AaipzLE7CCZTAaZzDxK3u2aN8TVjDta267d+BPuLk8eSKkW1CgsKq7J0KiWOdlZQW5tifuPSgfcfn48DZt+Sdfsd7S1wvxBzbFw3+9IvpVrrDCpCgoKiiCVav/As5BKNeP9KiPw7zXpyWyTlVOntAd7njx5Ek2aNIGFhUWVzvfz88OpU6cwevRorWtQqTeHdseboSvxxdcH0K97G1xIzsC3P57EB9NfBQA8yi/EF1sOIMC/OZwdFbifk4evfziB23/moG83cQ3WNjfWllJ4/FUlAQA3hTW8nWzxsKAYD/OL8fpzKvz8x13cf1QEd4UMY/29kJmdr5lBdCdX++Wnj4tKB9hmZefjbp72PjIt/h2bYdN3R+BSzwENVS5IScvEtl0/Y0BABwDA4/xC/Pe7w/jXc35wqmuP7JxHiNt7CnfuPUTPf7U0cvTiJ5EAUjOdDWS2yUp6ejrCwsIwceJEnDlzBitWrMAnn3xS5fOnTZuGMWPGoGPHjujSpQs2b96MixcvPnGArTlp5avC0rlBWLZ+D9Zs3o/6bo4If3sIXnqhtK9tIZUg7fpt/PBhAu7n5MFBboeWTRtgwyeT4cNZAyatiYs9Fg5poVkf36UhAGD/5dtYdTQNDR1t0cvXGXZWFriXV4iz17Ox6ZcMFKuf/Ns3mb5pwS/hy9j9WLr2B9zPyUO9unIMevE5BL0aAACQSiVIv/EnfjqyBdk5j6CQ26JZ4/pY8eE4NFK5PuXq9DRSPZMVfc41NrNNVkaPHo3Hjx/j+eefh4WFBaZNm4YJEyZU+fzhw4cjNTUVM2fORH5+PgIDAzFp0iT89NNPNRi1uPTs3Bw9OzevcJ/Mqg6WzQ2q5YjIEC7czMHA1fGV7q/oSbVPcvthwROvR6bD1kaGd94aiHfeGljhfplVHXw48/VajorMgVkmK4cPH9Z8vXr16nL7r169qrVeWT/23//+N/79739rbfvPf/6jd3xERET/xBcZ6ujYsWN444034O/vjxs3Sqcfbtq0CcePHzdocERERFSqrA2kzyJWOicr3333Hfr27QsbGxucPXtW85Cz7OxsfPTRRwYPkIiIiGrf0aNHMWjQIHh4eEAikSAuLk6zr6ioCOHh4WjVqhXs7Ozg4eGB0aNH4+bNm1rXaNiwYbnp0wsXLtQ5Fp2Tlfnz52PNmjVYu3at1vtxunTpgjNnzugcABERET1d2buB9Fl0kZeXhzZt2mDVqlXl9j169AhnzpxBREQEzpw5g+3btyM5ORmDBw8ud+y8efOQmZmpWd555x2dP7vOY1aSk5PRvXv3ctuVSiUePHigcwBERET0dLX91uX+/fujf//+Fe5TKpXYt2+f1raVK1fi+eefR3p6Ojw9PTXb5XI53Nz0m+Wpc2XFzc0NKSkp5bYfP36c03aJiIhqiNQAC1D6Xrq/L39/Z50+srOzIZFI4ODgoLV94cKFcHJyQrt27bB48WIUF+v+gECdk5Xx48dj2rRpOHXqFCQSCW7evInNmzdjxowZmDRpks4BEBERUe1RqVRQKpWaJTo6Wu9r5ufnIzw8HCNHjoRCodBsnzp1KmJjY3Ho0CFMnDgRH330EWbOnKnz9XVuA82aNQtqtRq9evXCo0eP0L17d8hkMsyYMaNafSgiIiJ6uuqMO/nn+QCQkZGhlVDo+xqYoqIiDBs2DIIglHscSFhYmObr1q1bw8rKChMnTkR0dLRO99U5WZFIJJgzZw7ee+89pKSkIDc3F82bN4e9Pd+mSUREVFOk0HPMCkrPVSgUWsmKPsoSlWvXruHgwYNPvW6nTp1QXFyMq1evwtfXt8r3qfZD4aysrNC8ecVPJyUiIqJnW1micuXKFRw6dAhOTk5PPScxMRFSqRQuLi463UvnZCUgIOCJT8E7ePCgrpckIiKipzBUG6iqcnNztSbUpKWlITExEY6OjnB3d8err76KM2fOYNeuXSgpKUFWVhYAwNHREVZWVoiPj8epU6cQEBAAuVyO+Ph4hIaG4o033kDdunV1ikXnZKVt27Za60VFRUhMTMRvv/2GoCC+64WIiKgm1PaLDBMSEhAQEKBZLxt/EhQUhMjISPzwww8AyucFhw4dQs+ePSGTyRAbG4vIyEgUFBSgUaNGCA0N1RrHUlU6JytLliypcHtkZCRyc3N1DoCIiIhMT8+ePSt9Nx5Q+XvzyrRv3x4nT540SCzVejdQRd544w189dVXhrocERER/Y1E8r8Hw1VnEfF7DA331uX4+HhYW1sb6nJERET0N7U9ZsWU6JysDB06VGtdEARkZmYiISEBERERBguMiIiICKhGsqJUKrXWpVIpfH19MW/ePPTp08dggREREdH/1PYAW1OiU7JSUlKCt956C61atdJ52hERERFVn+Sv/+lzvljpNMDWwsICffr04duViYiIallZZUWfRax0ng3UsmVL/PHHHzURCxEREVE5Oicr8+fPx4wZM7Br1y5kZmaWe9U0ERERGZ45V1aqPGZl3rx5ePfddzFgwAAAwODBg7Ueuy8IAiQSCUpKSgwfJRERkZmTSCRPfN1NVc4XqyonK1FRUXj77bdx6NChmoyHiIiISEuVk5Wyx+r26NGjxoIhIiKiinHqchWJuYREREQkZnyCbRU1bdr0qQnLvXv39AqIiIiI6O90SlaioqLKPcGWiIiIal7ZCwn1OV+sdEpWRowYARcXl5qKhYiIiCphzmNWqvycFY5XISIiImPQeTYQERERGYGeA2xF/GqgqicrarW6JuMgIiKiJ5BCAqkeGYc+5xqbTmNWiIiIyDjMeeqyzu8GIiIiIqpNrKwQERGJgDnPBmKyQkREJALm/JwVtoGIiIjIpLGyQkREJALmPMCWyQoREZEISKFnG0jEU5fZBiIiIiKTxsoKERGRCLANRERERCZNCv3aIWJupYg5diIiIjIDrKwQERGJgEQigUSPXo4+5xobkxUiIiIRkEC/FyeLN1VhskJERCQKfIItERERkYliZYWIiEgkxFsb0Q+TFSIiIhEw5+essA1EREREJo2VFSIiIhHg1GUiIiIyaXyCLREREZGJYrJCREQkAmVtIH0WXRw9ehSDBg2Ch4cHJBIJ4uLitPYLgoC5c+fC3d0dNjY26N27N65cuaJ1zL179zBq1CgoFAo4ODggODgYubm5On92JitEREQiIDHAoou8vDy0adMGq1atqnD/okWLsHz5cqxZswanTp2CnZ0d+vbti/z8fM0xo0aNwsWLF7Fv3z7s2rULR48exYQJE3SMhGNWiIiIqAL9+/dH//79K9wnCAKWLl2K999/H0OGDAEAbNy4Ea6uroiLi8OIESOQlJSEvXv34tdff0XHjh0BACtWrMCAAQPw8ccfw8PDo8qxMFkxAS4KGRQKmbHDoBr22WttjB0C1SL/mXHGDoFqgVD4qNbuZajZQDk5OVrbZTIZZDLdfgalpaUhKysLvXv31mxTKpXo1KkT4uPjMWLECMTHx8PBwUGTqABA7969IZVKcerUKbzyyitVvh/bQERERCIgNcACACqVCkqlUrNER0frHEtWVhYAwNXVVWu7q6urZl9WVhZcXFy09ltaWsLR0VFzTFWxskJERCQChqqsZGRkQKFQaLbrWlUxBlZWiIiIzIhCodBaqpOsuLm5AQBu3bqltf3WrVuafW5ubrh9+7bW/uLiYty7d09zTFUxWSEiIhKB2p4N9CSNGjWCm5sbDhw4oNmWk5ODU6dOwd/fHwDg7++PBw8e4PTp05pjDh48CLVajU6dOul0P7aBiIiIRKC2X2SYm5uLlJQUzXpaWhoSExPh6OgIT09PTJ8+HfPnz0eTJk3QqFEjREREwMPDAy+//DIAwM/PD/369cP48eOxZs0aFBUVISQkBCNGjNBpJhDAZIWIiIgqkJCQgICAAM16WFgYACAoKAgxMTGYOXMm8vLyMGHCBDx48ABdu3bF3r17YW1trTln8+bNCAkJQa9evSCVShEYGIjly5frHAuTFSIiIhGQQgKpHs0cXc/t2bMnBEGodL9EIsG8efMwb968So9xdHTEli1bdLpvRZisEBERiUBtt4FMCQfYEhERkUljZYWIiEgEJH/9T5/zxYrJChERkQiwDURERERkolhZISIiEgGJnrOB2AYiIiKiGmXObSAmK0RERCJgzskKx6wQERGRSWNlhYiISAQ4dZmIiIhMmlRSuuhzvlixDUREREQmjZUVIiIiEWAbiIiIiEwaZwMRERERmShWVoiIiERAAv1aOSIurDBZISIiEgPOBiIiIiIyUaysEBERiQBnAxEREZFJM+fZQExWiIiIREAC/QbJijhX4ZgVIiIiMm2srBAREYmAFBJI9ejlSEVcW2GyQkREJAJsAxERERGZKFZWiIiIxMCMSytMVoiIiETAnJ+zwjYQERERmTRWVoiIiMRAz4fCibiwwmSFiIhIDMx4yArbQERERGTaWFkhIiISAzMurTBZISIiEgFzng3EZIWIiEgEzPmtyxyzQkRERCaNlRUiIiIRMOMhK0xWiIiIRMGMsxW2gYiIiMiksbJCREQkAuY8G4iVFSIiIhEomw2kz6KLhg0bQiKRlFumTJkCAOjZs2e5fW+//XYNfHJWVoiIiKgCv/76K0pKSjTrv/32G1588UW89tprmm3jx4/HvHnzNOu2trY1EguTFSIiIhEw1PjanJwcre0ymQwymazc8c7OzlrrCxcuROPGjdGjRw/NNltbW7i5uekRVdWwDURERCQGEgMsAFQqFZRKpWaJjo5+6q0LCwvx3//+F2PHjoXkb/2kzZs3o169emjZsiVmz56NR48eGerTamFlhYiIyIxkZGRAoVBo1iuqqvxTXFwcHjx4gDFjxmi2vf766/Dy8oKHhwfOnz+P8PBwJCcnY/v27QaPmckKERGRCBhqNpBCodBKVqriyy+/RP/+/eHh4aHZNmHCBM3XrVq1gru7O3r16oXU1FQ0bty42nFWhG0gIiIiEajt2UBlrl27hv3792PcuHFPPK5Tp04AgJSUlOrd6AlYWSEiIhIBYz3Adv369XBxccHAgQOfeFxiYiIAwN3dvZp3qhyTFSIiIqqQWq3G+vXrERQUBEvL/6UMqamp2LJlCwYMGAAnJyecP38eoaGh6N69O1q3bm3wOJisEBERiYERSiv79+9Heno6xo4dq7XdysoK+/fvx9KlS5GXlweVSoXAwEC8//77egRYOSYrVCuWbdyH+Z/txIThPbAgNBAAsDHuZ3z302mcT85A7qMCpOxbCKW8Zh4oRDXr1p/ZWPbVj/g5IRn5BYVQedRDVOhraNFUBQBo239mhedNDx6AMa/2rMVISRedmjpjcr/maNWwLtwcbDF2xVHsPXtds7+ewhpzXm2LHi3doLSxwsnfb+P9zaeRdvshAMDBzgozhrRCj5bu8HC0xb2HBdh79joW7TiPh4+LjPWxRMsYj9vv06cPBEEot12lUuHIkSPVjkVXTFaoxp29dA0bd/yMFj4eWtsf5RfiBX8/vODvh/mf7TRSdKSvnIePMObdz/Bcm8ZY+eFYOCrtce3Gn1DY/y/x3L85Quuc4wmXEbX0W/Tu0qq2wyUd2MoscTHjPr4+noqvQrqX2/9VSHcUl6jx1vKjyM0vwoQ+zbB1xgvo8f4uPC4sgauDDVwdbDBv61n8fjMbDZzssHD0c3B1sMGEz44b4RORWDFZMYDIyEjExcVpBhfR/+Q+KsDbH2zEp7NH4tP1P2nte3tEAADg59NXjBEaGcj6bw7DzVmJeWHDNNvquzlqHVPPUa61fvjkJTzXujEauDvVSoxUPYcuZOLQhcwK93m7ytHRpx56vr8bv9/MBgDM2vQrzi0Zilc6NcSWY6lIvpGN8X9LSq7dycV/tp/DivH/goVUghJ1+d/YqXL6zOgpO1+sOHWZalT4x9/gxS4t0ON5X2OHQjXkyMlLaN6kAWYs2ISAEVEYPmUpvttzqtLj795/iOO/JOHlvs/VYpRkaFaWpT8+Cor+9+4YQQAKi0vwXBPnyk6DwsYKuflFTFSqwUAPsBUlJit/UavVWLRoEXx8fCCTyeDp6YkFCxYAAMLDw9G0aVPY2trC29sbERERKCoq7bfGxMQgKioK586d07x1MiYmpsJ7FBQUICcnR2t5lu3YdxoXkjPw/qRBxg6FatD1rHv4ZvdJeNavh9Xzx+G1gZ2xaM33+GFfQoXH/7D/NGxtZOjVpWUtR0qGlJKVg+t/5mH2q22gtK2DOhZSTOnvBw9HO7g62FR4jqO9DNMHtcR/jxj+ORz0bGMb6C+zZ8/G2rVrsWTJEnTt2hWZmZm4fPkyAEAulyMmJgYeHh64cOECxo8fD7lcjpkzZ2L48OH47bffsHfvXuzfvx8AoFQqK7xHdHQ0oqKiau0zGdONW/cx59Pt+Gb5ZFjL6hg7HKpBakFA8yYNMHVMfwBAM5/6SL12C9/+eBKDX+xY7vjv/+9XDAhoB5kV/7sQs+ISAcGrjuLTtzojaeVrKC5R49ilLBw4f7PCdoO9tSU2Tu+B3zOz8cn3F2o/4GeBsR60YgKYrAB4+PAhli1bhpUrVyIoKAgA0LhxY3Tt2hUAtKZiNWzYEDNmzEBsbCxmzpwJGxsb2Nvbw9LS8qlvnpw9ezbCwsI06zk5OVCpVDXwiYzv3OUM3Ln/EL3GLNZsKylRIz4xFV9+eww3jn4KCwsW9p4Fzo5yNPZ00drWSOWC/T+X/4F05rc0XL1+B/+ZPaq2wqMadOHafbwYuQdymzqoYynFvYcF2PV+H5y/ek/rODtrS2wJC0BefjGCVxxFcQlbQNVhjNlApoLJCoCkpCQUFBSgV69eFe7funUrli9fjtTUVOTm5qK4uFjn9yoAlb+G+1nUvWNTHN08S2vb1Plb0MTLBe+82ZuJyjOkTfOGuHr9jta2azfuwN2lbrljd/z0C5o3qQ9fb49y+0i8yqYhN3KRo01DRyzecV6zz97aElvCXkBhcQnGLD+CgmK1scIkEeNPDAA2NhX3VwEgPj4eo0aNwoABA7Br1y6cPXsWc+bMQWFhYS1GKD72dtbwa+yhtdhaW6Gu0g5+jUt/UN26m4MLv1/HH3/9oLuUmokLv1/H/ew8Y4ZOOnrj5W64cDkd62IPIv3mn/jx0Fl8t+cUhr/kr3Vcbl4+9h07j1f6Pm+kSElXtjJLtFA5oIXKAQCgqmeHFioH1HcsnZb+UkcV/H1d4Olsh75t6yN2RgD2nrmOIxezAJQmKl+/+wJsZRZ4d/0p2FvXgbPCGs4Ka0jFPDXFSIz1biBTwMoKgCZNmsDGxgYHDhwo96KmEydOwMvLC3PmzNFsu3btmtYxVlZWKCkpAelmw/bjWPzlXs364LeXAQCWvz8KI1/qZKywSEctfVX4NGI0lsfsxRdb9qO+myPemzgYA19or3Xc3iOJAIB+PdvWfpBULW0aOuK78N6a9aiRHQAAW4//gdCvTsLVwQaRI9qjnsIatx/k45v4NCz94TfN8a28HNGhcT0AQPx/Bmtd+/n3vsf1u/zFRBdmPGQFEqGiR9OZoaioKCxbtgxLly5Fly5dcOfOHVy8eBHOzs4IDAzEpk2b8Nxzz2H37t2IiopCSUkJHjx4AADYsmULJkyYgOPHj6NBgwaQy+VVavfk5ORAqVTixu371WorkbjcuJ9v7BCoFvnPjDN2CFQLhMJHeLB1PLKzs2vs3/GynxWnr2TCXl79e+Q+zEGHJu41GmtNYRvoLxEREXj33Xcxd+5c+Pn5Yfjw4bh9+zYGDx6M0NBQhISEoG3btjhx4gQiIrSfxhkYGIh+/fohICAAzs7O+Prrr430KYiIiJ49rKwYESsr5oWVFfPCyop5qM3KypkrWXpXVto3cRNlZYVjVoiIiMRA30GyIh60wjYQERERmTRWVoiIiETAnGcDMVkhIiISAzPOVtgGIiIiIpPGygoREZEI8N1AREREZNL0fWS+mB+3zzYQERERmTRWVoiIiETAjMfXMlkhIiISBTPOVpisEBERiYA5D7DlmBUiIiIyaaysEBERiYAEes4GMlgktY/JChERkQiY8ZAVtoGIiIjItLGyQkREJALm/FA4JitERESiYL6NILaBiIiIyKSxskJERCQCbAMRERGRSTPfJhDbQERERGTiWFkhIiISAbaBiIiIyKSZ87uBmKwQERGJgRkPWuGYFSIiIjJprKwQERGJgBkXVlhZISIiEoOyAbb6LLqIjIyERCLRWpo1a6bZn5+fjylTpsDJyQn29vYIDAzErVu3DPypSzFZISIiogq1aNECmZmZmuX48eOafaGhodi5cye++eYbHDlyBDdv3sTQoUNrJA62gYiIiETAGLOBLC0t4ebmVm57dnY2vvzyS2zZsgUvvPACAGD9+vXw8/PDyZMn0blz52rHWRFWVoiIiMRAYoAFQE5OjtZSUFBQ6S2vXLkCDw8PeHt7Y9SoUUhPTwcAnD59GkVFRejdu7fm2GbNmsHT0xPx8fEG/dgAkxUiIiKzolKpoFQqNUt0dHSFx3Xq1AkxMTHYu3cvVq9ejbS0NHTr1g0PHz5EVlYWrKys4ODgoHWOq6srsrKyDB4z20BEREQiYKjZQBkZGVAoFJrtMpmswuP79++v+bp169bo1KkTvLy8sG3bNtjY2OgRie5YWSEiIhIBQ80GUigUWktlyco/OTg4oGnTpkhJSYGbmxsKCwvx4MEDrWNu3bpV4RgXfTFZISIioqfKzc1Famoq3N3d0aFDB9SpUwcHDhzQ7E9OTkZ6ejr8/f0Nfm+2gYiIiERBv9lAujaRZsyYgUGDBsHLyws3b97EBx98AAsLC4wcORJKpRLBwcEICwuDo6MjFAoF3nnnHfj7+xt8JhDAZIWIiEgUavuty9evX8fIkSNx9+5dODs7o2vXrjh58iScnZ0BAEuWLIFUKkVgYCAKCgrQt29ffPbZZ9UP8AmYrBAREVE5sbGxT9xvbW2NVatWYdWqVTUeC8esEBERkUljZYWIiEgEarsNZEqYrBAREYmAMR63byrYBiIiIiKTxsoKERGRCLANRERERCbNUI/bFyO2gYiIiMiksbJCREQkBmZcWmGyQkREJAKcDURERERkolhZISIiEgHOBiIiIiKTZsZDVpisEBERiYIZZyscs0JEREQmjZUVIiIiETDn2UBMVoiIiESAA2zJKARBAAA8fJhj5EioNuQ+zDd2CFSLhMJHxg6BaoFQ9Lj0///697wm5eTo97NC3/ONicmKET18+BAA0Kyxl5EjISIifTx8+BBKpbJGrm1lZQU3Nzc0aaTS+1pubm6wsrIyQFS1SyLURjpIFVKr1bh58ybkcjkkYq7P6SgnJwcqlQoZGRlQKBTGDodqEL/X5sNcv9eCIODhw4fw8PCAVFpzc1by8/NRWFio93WsrKxgbW1tgIhqFysrRiSVStGgQQNjh2E0CoXCrP5RM2f8XpsPc/xe11RF5e+sra1FmWQYCqcuExERkUljskJEREQmjckK1TqZTIYPPvgAMpnM2KFQDeP32nzwe001iQNsiYiIyKSxskJEREQmjckKERERmTQmK0RERGTSmKxQlVy9ehUSiQSJiYl6X2vMmDF4+eWX9b4OGUfPnj0xffr0Svc3bNgQS5cu1fm6kZGRaNu2bbXjIqJnFx8KR1WiUqmQmZmJevXqGTsUMnG//vor7OzsjB0GmZDIyEjExcUZ5JcdMk9MVqhKLCws4ObmVul+QRBQUlICS0v+J2XunJ2dn7i/qKgIderUqaVoiOhZwDYQaezduxddu3aFg4MDnJyc8NJLLyE1NRVA+TbQ4cOHIZFIsGfPHnTo0AEymQzHjx/XlPI///xzqFQq2NraYtiwYcjOzq7Wff9+7+3btyMgIAC2trZo06YN4uPjta5z/PhxdOvWDTY2NlCpVJg6dSry8vIM/wdFKC4uRkhICJRKJerVq4eIiAjNW2f/2QaSSCRYvXo1Bg8eDDs7OyxYsAAAsHDhQri6ukIulyM4OBj5+XwrtSlTq9VYtGgRfHx8IJPJ4OnpqflehoeHo2nTprC1tYW3tzciIiJQVFQEAIiJiUFUVBTOnTsHiUQCiUSCmJgYI34SEiMmK6SRl5eHsLAwJCQk4MCBA5BKpXjllVegVqsrPWfWrFlYuHAhkpKS0Lp1awBASkoKtm3bhp07d2Lv3r04e/YsJk+erPd958yZgxkzZiAxMRFNmzbFyJEjUVxcDABITU1Fv379EBgYiPPnz2Pr1q04fvw4QkJCDPAnQ/+0YcMGWFpa4pdffsGyZcvw6aefYt26dZUeHxkZiVdeeQUXLlzA2LFjsW3bNkRGRuKjjz5CQkIC3N3d8dlnn9XiJyBdzZ49GwsXLkRERAQuXbqELVu2wNXVFQAgl8sRExODS5cuYdmyZVi7di2WLFkCABg+fDjeffddtGjRApmZmcjMzMTw4cON+VFIjASiSty5c0cAIFy4cEFIS0sTAAhnz54VBEEQDh06JAAQ4uLitM754IMPBAsLC+H69euabXv27BGkUqmQmZkpCIIgBAUFCUOGDKnSfQVB0Nx73bp1mmMuXrwoABCSkpIEQRCE4OBgYcKECVrXOXbsmCCVSoXHjx9X+8+AyuvRo4fg5+cnqNVqzbbw8HDBz89PEARB8PLyEpYsWaLZB0CYPn261jX8/f2FyZMna23r1KmT0KZNmxqLm6ovJydHkMlkwtq1a6t0/OLFi4UOHTpo1j/44AN+b0kvrKyQxpUrVzBy5Eh4e3tDoVCgYcOGAID09PRKz+nYsWO5bZ6enqhfv75m3d/fH2q1GsnJyXrdt6xyAwDu7u4AgNu3bwMAzp07h5iYGNjb22uWvn37Qq1WIy0t7ekfnnTSuXNnSCQSzbq/vz+uXLmCkpKSCo//538nSUlJ6NSpk9Y2f39/wwdKBpGUlISCggL06tWrwv1bt25Fly5d4ObmBnt7e7z//vtP/HeDSFccDUkagwYNgpeXF9auXQsPDw+o1Wq0bNkShYWFlZ5jiFkfVb3v3wdllv2gLGsV5ebmYuLEiZg6dWq563t6euodI+mHs4PEzcbGptJ98fHxGDVqFKKiotC3b18olUrExsbik08+qcUI6VnHZIUAAHfv3kVycjLWrl2Lbt26ASgdsFod6enpuHnzJjw8PAAAJ0+ehFQqha+vb43dt3379rh06RJ8fHyqFTPp5tSpU1rrJ0+eRJMmTWBhYVGl8/38/HDq1CmMHj1a6xpkmpo0aQIbGxscOHAA48aN09p34sQJeHl5Yc6cOZpt165d0zrGysqq0qobUVUwWSEAQN26deHk5IQvvvgC7u7uSE9Px6xZs6p1LWtrawQFBeHjjz9GTk4Opk6dimHDhlU49dlQ9w0PD0fnzp0REhKCcePGwc7ODpcuXcK+ffuwcuXKan0Oqlx6ejrCwsIwceJEnDlzBitWrNDpN+lp06ZhzJgx6NixI7p06YLNmzfj4sWL8Pb2rsGoqbqsra0RHh6OmTNnwsrKCl26dMGdO3dw8eJFNGnSBOnp6YiNjcVzzz2H3bt3Y8eOHVrnN2zYEGlpaUhMTESDBg0gl8v5dmbSCcesEABAKpUiNjYWp0+fRsuWLREaGorFixdX61o+Pj4YOnQoBgwYgD59+qB169aVzvQw1H1bt26NI0eO4Pfff0e3bt3Qrl07zJ07V1PdIcMaPXo0Hj9+jOeffx5TpkzBtGnTMGHChCqfP3z4cERERGDmzJno0KEDrl27hkmTJtVgxKSviIgIvPvuu5g7dy78/PwwfPhw3L59G4MHD0ZoaChCQkLQtm1bnDhxAhEREVrnBgYGol+/fggICICzszO+/vprI30KEiuJIPz1cAQiA+CTKomIyNBYWSEiIiKTxmSFiIiITBrbQERERGTSWFkhIiIik8ZkhYiIiEwakxUiIiIyaUxWiIiIyKQxWSEiIiKTxmSFyMyNGTMGL7/8sma9Z8+emD59eq3HcfjwYUgkEjx48KDSYyQSCeLi4qp8zcjISLRt21avuK5evQqJRMIHHRIZEZMVIhM0ZswYSCQSSCQSWFlZwcfHB/PmzUNxcXGN33v79u348MMPq3RsVRIMIiJ98UWGRCaqX79+WL9+PQoKCvDjjz9iypQpqFOnDmbPnl3u2MLCQlhZWRnkvo6Ojga5DhGRobCyQmSiZDIZ3Nzc4OXlhUmTJqF379744YcfAPyvdbNgwQJ4eHjA19cXAJCRkYFhw4bBwcEBjo6OGDJkCK5evaq5ZklJCcLCwuDg4AAnJyfMnDkT/3wu5D/bQAUFBQgPD4dKpYJMJoOPjw++/PJLXL16FQEBAQBK354tkUgwZswYAIBarUZ0dDQaNWoEGxsbtGnTBt9++63WfX788Uc0bdoUNjY2CAgI0IqzqsLDw9G0aVPY2trC29sbERERKCoqKnfc559/DpVKBVtbWwwbNgzZ2dla+9etWwc/Pz9YW1ujWbNmlb54k4iMg8kKkUjY2NigsLBQs37gwAEkJydj37592LVrF4qKitC3b1/I5XIcO3YMP//8M+zt7dGvXz/NeZ988gliYmLw1Vdf4fjx47h37x527NjxxPuOHj0aX3/9NZYvX46kpCR8/vnnsLe3h0qlwnfffQcASE5ORmZmJpYtWwYAiI6OxsaNG7FmzRpcvHgRoaGheOONN3DkyBEApUnV0KFDMWjQICQmJmLcuHGYNWuWzn8mcrkcMTExuHTpEpYtW4a1a9diyZIlWsekpKRg27Zt2LlzJ/bu3YuzZ89i8uTJmv2bN2/G3LlzsWDBAiQlJeGjjz5CREQENmzYoHM8RFRDBCIyOUFBQcKQIUMEQRAEtVot7Nu3T5DJZMKMGTM0+11dXYWCggLNOZs2bRJ8fX0FtVqt2VZQUCDY2NgIP/30kyAIguDu7i4sWrRIs7+oqEho0KCB5l6CIAg9evQQpk2bJgiCICQnJwsAhH379lUY56FDhwQAwv379zXb8vPzBVtbW+HEiRNaxwYHBwsjR44UBEEQZs+eLTRv3lxrf3h4eLlr/RMAYceOHZXuX7x4sdChQwfN+gcffCBYWFgI169f12zbs2ePIJVKhczMTEEQBKFx48bCli1btK7z4YcfCv7+/oIgCEJaWpoAQDh79myl9yWimsUxK0QmateuXbC3t0dRURHUajVef/11REZGava3atVKa5zKuXPnkJKSArlcrnWd/Px8pKamIjs7G5mZmejUqZNmn6WlJTp27FiuFVQmMTERFhYW6NGjR5XjTklJwaNHj/Diiy9qbS8sLES7du0AAElJSVpxAIC/v3+V71Fm69atWL58OVJTU5Gbm4vi4mIoFAqtYzw9PVG/fn2t+6jVaiQnJ0MulyM1NRXBwcEYP3685pji4mIolUqd4yGimsFkhchEBQQEYPXq1bCysoKHhwcsLbX/utrZ2Wmt5+bmokOHDti8eXO5azk7O1crBhsbG53Pyc3NBQDs3r1bK0kASsfhGEp8fDxGjRqFqKgo9O3bF0qlErGxsfjkk090jnXt2rXlkicLCwuDxUpE+mGyQmSi7Ozs4OPjU+Xj27dvj61bt8LFxaVcdaGMu7s7Tp06he7duwMorSCcPn0a7du3r/D4Vq1aQa1W48iRI+jdu3e5/WWVnZKSEs225s2bQyaTIT09vdKKjJ+fn2awcJmTJ08+/UP+zYkTJ+Dl5YU5c+Zotl27dq3ccenp6bh58yY8PDw095FKpfD19YWrqys8PDzwxx9/YNSoUTrdn4hqDwfYEj0jRo0ahXr16mHIkCE4duwY0tLScPjwYUydOhXXr18HAEybNg0LFy5EXFwcLl++jMmTJz/xGSkNGzZEUFAQxo4di7i4OM01t23bBgDw8vKCRCLBrl27cOfOHeTm5kIul2PGjBkIDQ3Fhg0bkJqaijNnzmDFihWaQatvv/02rly5gvfeew/JycnYsmULYmJidPq8TZo0QXp6OmJjY5Gamorly5dXOFjY2toaQUFBOHfuHI4dO4apU6di2LBhcHNzAwBERUUhOjoay5cvx++//44LFy5g/fr1+PTTT3WKh4hqDpMVomeEra0tjh49Ck9PTwwdOhR+fn4IDg5Gfn6+ptLy7rvv4s0330RQUBD8/f0hl8vxyiuvPPG6q1evxquvvorJkyejWbNmGD9+PPLy8gAA9evXR1RUFGbNmgVXV1eEhIQAAD788ENEREQgOjoafn5+6NevH3bv3o1GjRoBKB1H8t133yEuLg5t2rTBmjVr8NFHH+n0eQcPHozQ0FCEhISgbdu2OHHiBCIiIsod5+Pjg6FDh2LAgAHo06cPWrdurTU1edy4cVi3bh3Wr1+PVq1aoUePHoiJidHESkTGJxEqG1lHREREZAJYWSEiIiKTxmSFiIiITBqTFSIiIjJpTFaIiIjIpDFZISIiIpPGZIWIiIhMGpMVIiIiMmlMVoiIiMikMVkhIiIik8ZkhYiIiEwakxUiIiIyaf8PEConYz6cjncAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_test_pred, labels=['airplane', 'bird', 'cat'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['airplane', 'bird', 'cat'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc6366c-d7f2-4a9b-8c8e-08cfad39f3b7",
      "metadata": {
        "id": "ccc6366c-d7f2-4a9b-8c8e-08cfad39f3b7"
      },
      "source": [
        "## Build a CNN (6 Points)\n",
        "\n",
        "In this section, you should construct and validate a CNN architecture for the given problem.\n",
        "\n",
        "It's required that you test at least the following parts of a CNN:\n",
        "- Number of convolutional layers and different output sizes\n",
        "- Kernel size of filters\n",
        "- Number of training epochs\n",
        "- Learning rate of the network\n",
        "- Batchsize used in dataloader\n",
        "- Stride on convolutional or pooling layers\n",
        "- Different architecture for the final dense layer (with or without hidden layers and their size)\n",
        "\n",
        "Also, you must compare at least for one network if the data augmentation is helping the training process or not.\n",
        "\n",
        "Lastly, you must change the `Criterion` function to return the balanced accuracy instead of the normal one.\n",
        "\n",
        "--------\n",
        "\n",
        "The process to build a CNN is an exploratory analysis and should be done carefully. As this is a costly process, you will not test all possibilities between each other, you should interpret the results at each step and understand what is happening or not happening.\n",
        "\n",
        "As this is a costly process, you should use the normal division of train/validation/test, instead of cross validation. Also, at each network built and trained/validated, you should look at the results to understand if any overfitting or underfitting is happening. Save the values of each batch of train and validation to analyze how the model performed over the epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eL43xkO2e41S",
      "metadata": {
        "id": "eL43xkO2e41S"
      },
      "source": [
        "### Exploding/vanishing gradients\n",
        "\n",
        "As we already learned, the weights of a neural network are updated using the backpropagation algorithm. As our network grows in depth, there are two problems that can occur with the backpropagation algorithm, known as Exploding and Vanishing gradients.\n",
        "\n",
        "As both names imply, both conditions relate to a gradient calculation that goes out of control for some reason.\n",
        "\n",
        "The exploding gradient can happen when the current weights generate a very large loss, and all the network weights are updated by a huge amount. This can lead to a cycle where the network is not able to learn because every learning step impacts too much the network, making it very unstable.\n",
        "\n",
        "On the other side, the vanishing gradient will happen when the gradients get too small, and the updates are not able to propagate to the initial layers.\n",
        "\n",
        "There are a couple of ways to solve those issues. The first is to use an activation function that is \"non-saturating\". The sigmoid function is an example of a saturating function because its derivative tends to 0 on larger positive or negative values. The best activation functions to avoid this issue are the ReLU and its variations (LReLU, PReLU, ELU, etc). All those examples help to mitigate the vanishing gradient.\n",
        "\n",
        "Another important step is to initialize the weights of the network properly. Initializing the weights randomly can also lead to vanishing/exploding gradients in some situations. The common strategy is to use a heuristic. We are not going to cover this manner in a deeper aspect, but the Xavier initialization is already implemented in the example below.\n",
        "\n",
        "Finally, the best thing to avoid vanishing and exploding gradients in a deep CNN is to use the Batch Normalization technique. This technique is a new operation on the CNN, that normalizes its input and adds a parameter for scale and another for shifting. This enables the network to learn the optimal scale and mean of the layer. This normalization uses the data of the batch of images being executed on the model to find the mean and standard deviation. In pytorch, there is a function to use this technique in the `nn` module: `nn.BatchNorm2d()`. The BatchNorm operation can be used before or after the activation function.\n",
        "\n",
        "For example:\n",
        "- Conv -> BatchNorm -> Activation(ReLU) -> Pooling\n",
        "Or\n",
        "- Conv -> Activation(ReLU) -> BatchNorm -> Pooling).\n",
        "\n",
        "In this task, you may face problems concerning the stability and convergence of the network because of the exploding or vanishing gradients, and it is recommended that you use the techniques cited to avoid this issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "Ru9eRQukb8mJ",
      "metadata": {
        "id": "Ru9eRQukb8mJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU not available!\n"
          ]
        }
      ],
      "source": [
        "gpu = torch.cuda.is_available()\n",
        "\n",
        "if not gpu:\n",
        "  print(\"GPU not available!\")\n",
        "\n",
        "device = torch.device(0) if gpu else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BD2bBcJ6eCzU",
      "metadata": {
        "id": "BD2bBcJ6eCzU"
      },
      "source": [
        "**Saving models**\n",
        "\n",
        "As the CNN training process can take some time, is interesting to save your models, to avoid re-training in the case of a crash or just to save your work for other time.\n",
        "\n",
        "The next cell shows a simple code to save and load pytorch models. Remember to keep the class of the model in the same way, as the state_dict will not work if you change the architecture. Create new classes for different tests, with names that correctly describe what is being tested.\n",
        "\n",
        "**Note**: If you're using google colab, just saving the model is not enough. You need to save it for a folder in your google Drive, or download the files after saving them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c2738d8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIMPLIFICANDO EM UMA FUNÇÃO PARA PODER CHAMAR PARA DIFERENTES MÉTODOS\n",
        "\n",
        "def load_model(model):\n",
        "    \n",
        "    #model = Model(input_shape, nclasses)\n",
        "    state_dict  = torch.load(\"/savedModels\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(device)\n",
        "    return\n",
        "\n",
        "def save_model(model):\n",
        "    torch.save(model.to('cpu').state_dict(), \"/savedModels\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E832WhmRV8Fw",
      "metadata": {
        "id": "E832WhmRV8Fw"
      },
      "source": [
        "**Example of train/validate code**\n",
        "\n",
        "In the next cells, we show an example of how to develop the train/validation functions using pytorch. You don't need to use this code, but it is a place to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "i-_hHXzcV70s",
      "metadata": {
        "id": "i-_hHXzcV70s"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_batch(model, data, optimizer, criterion, device):\n",
        "  '''\n",
        "  Funtion that trains a batch of data in the 'device' (that should be a GPU)\n",
        "  'data' is a batch of the dataloader.\n",
        "  'optmizer' should be an instance of an pytorch optmizer. You don't need to test different optmizers, just go with SGD (torch.optim.SGD).\n",
        "  'criterion' is a custom function to calculate the loss and accuracy of the predictions of the batch.\n",
        "  'device' is the device that the model is loaded on.\n",
        "  '''\n",
        "  model.train()\n",
        "  ims, targets = data\n",
        "  ims = ims.to(device=device)\n",
        "  targets = targets.to(device=device)\n",
        "  preds = model(ims)\n",
        "  loss, acc = criterion(model, preds, targets, device)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  return loss.item(), acc.item()\n",
        "\n",
        "# This line avoid that the model weights change during validation\n",
        "@torch.no_grad()\n",
        "def validate_batch(model, data, criterion, device):\n",
        "  '''\n",
        "  Funtion that validates a batch of data in the 'device' (that should be a GPU).\n",
        "  Same parameters as 'train_batch', but this time no optmizer is needed.\n",
        "  '''\n",
        "  model.eval()\n",
        "  ims, targets = data\n",
        "  ims = ims.to(device=device)\n",
        "  targets = targets.to(device=device)\n",
        "  preds = model(ims)\n",
        "  loss, acc = criterion(model, preds, targets, device)\n",
        "\n",
        "  return loss.item(), acc.item()\n",
        "\n",
        "def Criterion(model, preds, targets, device):\n",
        "  '''\n",
        "  Function that calculates the loss and accuracy of a batch predicted by the model.\n",
        "  '''\n",
        "  ce            = nn.CrossEntropyLoss().to(device) # You don't need to change the loss function (but you can if it makes sense on your analysis)\n",
        "  loss          = ce(preds, targets.long())\n",
        "  pred_labels   = torch.max(preds.data, 1)[1] # same as argmax\n",
        "  acc           = torch.sum(pred_labels == targets.data)\n",
        "  n             = pred_labels.size(0)\n",
        "  acc           = acc/n\n",
        "\n",
        "  return loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188de6c1",
      "metadata": {},
      "source": [
        "## Building a CNN\n",
        "\n",
        "### Modelando a Rede Neural : Layer Base\n",
        "\n",
        "Queremos criar uma Modelo de CNN que possa ser reutilizável para diferentes parâmetros, para assim facilitar os testes de qual hiperparâmetro melhorou os testes.\n",
        "\n",
        "Primeiramente, vamos definir um Modelo de um dos Layers que iremos aplicar na Rede Neural.\n",
        "Esse Layer será definido com os seguintes passos:\n",
        "\n",
        "Input Layers -> [ Convolution Layer -> Batch Layer -> ReLu Layer -> MaxPool Layer ] -> Output Output \n",
        "\n",
        "Com isso, fazemos a convolução, utilizamos o Batch para tentar evitar **exploding/vanishing gradients**, em seguida aplicamos a função de ativação ReLU e fazemos o pooling com Max-Pooling para diminuir a extensão espacial da rede.\n",
        "\n",
        "Com a implementação abaixo conseguiremos chamar o BaseLayer e injetar hiperparâmetros diferentes, levando em conta a Convolução como a função principal de aplicação desses parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "cb8b063f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(BaseLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels= in_channels, out_channels= out_channels, kernel_size=kernel_size, stride= stride, padding= padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size= 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool(self.activation(self.bn(self.conv(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8749812",
      "metadata": {},
      "source": [
        "### Modelando a Rede Neural : Construção da CNN\n",
        "\n",
        "Agora que temos um Layer Base podemos utilizar vários desse modelo em sequência para treinar a CNN , sendo cada instanciação desse Layer podendo ter tamanhos, kernels , strides e paddings diferentes.\n",
        "\n",
        "Além disso, precisamos de uma Layer totalmente conectado para fazer o achatamento e a classificação final.\n",
        "\n",
        "Tentamos ao máximo deixar essa parte reutilizável, com isso colocamos em função de uma variável que será recebida chamada **hyperparameters**, tal vetor tem o seguinte formato:\n",
        "\n",
        "[ (input_1, output_1, kernel_1, stride_1, padding_1) , ... , (input_n, output_n, kernel_n, stride_n, padding_n)] \n",
        ", sendo **n** o número de replicações que faremos do modelo Base Layer. \n",
        "\n",
        "Podemos, então, injetar um vetor de hyperparametros para termos diversos layers de convolução, usando ou não o stride, diferentes tamanhos de kernel etc.\n",
        "\n",
        "Vale ressaltar, que o próximo layer tem dependência do que sair do anterior, logo não tem como colocar quaisquer valores de hyperparametros, eles tem que seguir uma lógica da rede neural, levando em consideração o tamanho do kernel, do padding, do stride e principalmente da input e output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "kyecR67TZ2YT",
      "metadata": {
        "id": "kyecR67TZ2YT"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, hyperparameters):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    \n",
        "rparameters[-1][1]\n",
        "\n",
        "    self.conv = nn.Sequential(*[BaseLayer(in_, out_, ker_, str_, pad_) \n",
        "                                for in_, out_, ker_, str_, pad_ in hyperparameters])\n",
        "    \n",
        "\n",
        "    self.output = nn.Sequential(nn.Dropout(0.5),\n",
        "                                nn.Linear(8192 // (2**(len(hyperparameters)-1)), 2*max_channel),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(2*max_channel, 100))\n",
        "    \n",
        "    # Inicializa os pesos\n",
        "    self._initialize_weights()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Executes a forward pass in the network with the input 'x'.\n",
        "    'x' can be a batch or a single image in the expected input_shape.\n",
        "    '''\n",
        "   \n",
        "    val = self.conv(x)\n",
        "    val = torch.flatten(val, start_dim=1)\n",
        "    return self.output(val)\n",
        "\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    '''\n",
        "    Initialize the network weights using the Xavier initialization.\n",
        "    '''\n",
        "    for x in self.modules():\n",
        "      if isinstance(x, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(x.weight.data)\n",
        "        if (x.bias is not None):\n",
        "          x.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354a4985",
      "metadata": {},
      "source": [
        "### Modelando a Rede Neural : Visualizando a Arquitetura\n",
        "\n",
        "Abaixo vemos uma forma de instanciar a CNN criada com um vetor de hiperparametros.\n",
        "\n",
        "No exemplo abaixo temos 4 layers com inputs e outputs aumentando, mas utilizando o mesmo número de kernel, stride e padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "c4ba2d64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 32, 16, 16]               0\n",
            "         BaseLayer-5           [-1, 32, 16, 16]               0\n",
            "            Conv2d-6           [-1, 64, 16, 16]          18,496\n",
            "       BatchNorm2d-7           [-1, 64, 16, 16]             128\n",
            "              ReLU-8           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-9             [-1, 64, 8, 8]               0\n",
            "        BaseLayer-10             [-1, 64, 8, 8]               0\n",
            "           Conv2d-11            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-12            [-1, 128, 8, 8]             256\n",
            "             ReLU-13            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-14            [-1, 128, 4, 4]               0\n",
            "        BaseLayer-15            [-1, 128, 4, 4]               0\n",
            "           Conv2d-16            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-17            [-1, 256, 4, 4]             512\n",
            "             ReLU-18            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-19            [-1, 256, 2, 2]               0\n",
            "        BaseLayer-20            [-1, 256, 2, 2]               0\n",
            "          Dropout-21                 [-1, 1024]               0\n",
            "           Linear-22                  [-1, 512]         524,800\n",
            "             ReLU-23                  [-1, 512]               0\n",
            "           Linear-24                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 965,476\n",
            "Trainable params: 965,476\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.66\n",
            "Params size (MB): 3.68\n",
            "Estimated Total Size (MB): 5.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Definindo o modelo\n",
        "hyperparameters = [(3,32,3,1,1),\n",
        "                    (32,64,3,1,1),\n",
        "                    (64,128,3,1,1),\n",
        "                    (128,256,3,1,1)]\n",
        "                      \n",
        "model = CNN(hyperparameters).to(device)\n",
        "\n",
        "summary(model, input_size=(3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58620e58",
      "metadata": {},
      "source": [
        "### Buscando o melhor Modelo \n",
        "\n",
        "Para melhor organização vamos separar como treinar os diferentes modelos e conseguir achar um melhor conjunto de hyperparametros\n",
        "\n",
        "- Number of convolutional layers and different output sizes\n",
        "    - Basta treinar um modelo com o vetor de hyperparameters com 1,2,3 Layers do Base Layer.\n",
        "\n",
        "- Kernel size of filters\n",
        "    - Basta treinar um modelo fixando um o número de convolutional layers e alterando apenas o tamanho do kernel_size\n",
        "\n",
        "- Number of training epochs\n",
        "    - Basta alterar a variável num_epochs criada no próxiom tópico e comparar 10, 100, 200 épocas\n",
        "\n",
        "- Learning rate of the network\n",
        "    - Basta fixar um modelo criado e alterar apenas a variável de learning rate do optmizer que é enviado para função train_batch\n",
        "\n",
        "- Batchsize used in dataloader\n",
        "    - Essa parte teremos que instanciar novamente os DataLoaders de treino, validação e teste. Para isso, podemos ficar um modelo e treinar apenas carregando diferentes batchsizes\n",
        "\n",
        "- Stride on convolutional or pooling layers\n",
        "    - Este parte teremos que fixar um conjunto de hyperparametros e apenas alterar a posição de stride, assim como o kernel_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131ea3e3",
      "metadata": {},
      "source": [
        "### Treinando uma Rede Neural\n",
        "\n",
        "Para essa parte criamos um código para testar a rede neural e ir acumulando seus resultados para diferentes epochs.\n",
        "Para começo utilizamos o optmizer com um learning rate de 0.01, mas depois podemos ir alterando e analisando um melhor valor para esse parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "IYFtXlG-bNu7",
      "metadata": {
        "id": "IYFtXlG-bNu7"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "# Using Stochastic Gradient Descent\n",
        "# Função de custo e otimizador\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2de6cd22",
      "metadata": {},
      "source": [
        "## ERRO\n",
        "\n",
        "Infelizmente ao rodar o código abaixo para treinar e validar o modelo criado acima está dando erro .\n",
        "\n",
        "Ao buscar possíveis soluções vimos que o problema acontece ao utilizar DataLoader com múltiplos processos (num_workers > 0).\n",
        "Porém , diminuíndo o número de num_workers ou zerando resultava no mesmo erro. \n",
        "\n",
        "Tentamos utilizar o computador do IC ao pensar ser uma limitação de hardware dos computadores pessoais, mas o computador do IC tem um limite muito baixo de memória e não conseguimos nem baixar as bibliotecas do torch.\n",
        "\n",
        "Enfim, acabamos travados por esse obstáculo que nos impediu de prosseguir.\n",
        "Achamos melhor documentar todo nosso processo até aqui para estudar e entender o funcionamento da CNN e como escalar o código para pode facilitar os testes, porém encontramos esse problema que não vimos solução até então.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "ab518c58",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 18468, 14836) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
            "\u001b[1;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[179], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\deasc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 18468, 14836) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "# Treinamento e validação\n",
        "num_epochs = 1\n",
        "\n",
        "train_results= []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    for data in trainLoader:\n",
        "        loss, acc = train_batch(model, data, optimizer, Criterion, device)\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(acc)\n",
        "\n",
        "    for data in validationLoader:\n",
        "        loss, acc = validate_batch(model, data, Criterion, device)\n",
        "        val_loss.append(loss)\n",
        "        val_acc.append(acc)\n",
        "\n",
        "    train_losses.append(np.mean(train_loss))\n",
        "    val_losses.append(np.mean(val_loss))\n",
        "    train_accuracies.append(np.mean(train_acc))\n",
        "    val_accuracies.append(np.mean(val_acc))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss_mean:.4f}, Validation Loss: {val_loss_mean:.4f}, Training Accuracy: {train_acc_mean:.4f}, Validation Accuracy: {val_acc_mean:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0077b9bd-3316-4554-9f98-9cfcab7a9177",
      "metadata": {
        "id": "0077b9bd-3316-4554-9f98-9cfcab7a9177"
      },
      "source": [
        "### Early stoping regularization (EXTRA: 1 Point)\n",
        "\n",
        "This can only be done if you are able to find a CNN that was able to overfit the train dataset.\n",
        "\n",
        "If that is the case, change the training function in order to perform an [early stopping](https://en.wikipedia.org/wiki/Early_stopping). The early stopping technique using validation is a technique to stop the training process when a defined condition is achieved. Find this condition and change the code.\n",
        "\n",
        "Discuss the results and why they happened (it works? Why?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf14ea2-42e3-46bd-be64-b1c38d7e83da",
      "metadata": {
        "id": "3cf14ea2-42e3-46bd-be64-b1c38d7e83da"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9d0bcc1a-a089-456c-9ea0-d79e3c8f54df",
      "metadata": {
        "id": "9d0bcc1a-a089-456c-9ea0-d79e3c8f54df"
      },
      "source": [
        "## Interpertability on CNNs (1 point)\n",
        "\n",
        "The Gradient-weighted Class Activation Mapping (Grad-CAM) technique uses the gradient of one or more convolutional layers to highlight the regions of the image that were more impactful to a prediction. For more details, you can read [this paper](https://arxiv.org/abs/1610.02391).\n",
        "\n",
        "The idea is to use the gradient to create a heatmap indicating the parts that had more weight on the decision for a given sample.\n",
        "\n",
        "In the next cells, the Grad-CAM code is defined for one layer. You should use this code to test a few images of your best CNN defined in the previous sections. You can change the visualizations if you want.\n",
        "\n",
        "Analyze the results of Grad-CAM for different images and classes, and discuss whether the model is focusing or not on the right parts of the image and why this may be happening. Feel free to change the code below to visualize in a different way (different quantity of images, different labeling, etc).\n",
        "\n",
        "-------\n",
        "\n",
        "**Important: The code below has some assumptions about the network architecture. If the model class is built in a different way you MUST change the implementation of the `get_activations` function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb5b97d-9554-4515-9970-22308dd2e77f",
      "metadata": {
        "id": "5fb5b97d-9554-4515-9970-22308dd2e77f"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels_map = {\n",
        "  0: \"Airplane\",\n",
        "  1: \"Bird\",\n",
        "  2: \"Cat\",\n",
        "}\n",
        "\n",
        "def get_activations(model, image, device):\n",
        "  '''\n",
        "  This function return the activations for a given image.\n",
        "  The assumption is that there is a \"features\" attribute that contains the convolutional part of the network.\n",
        "  If this is not the case of your network, change the code below to run the network until the last convolutional layer (with activations), and return it.\n",
        "  \"image\" is the selected image to get activations\n",
        "  \"device\" is the device where the model is running (to use the GPU when available)\n",
        "  '''\n",
        "  # move input tensor x to the selected device\n",
        "  image = image.to(device)\n",
        "  # get activations after feature extraction\n",
        "  img_activations = model.features(image)\n",
        "\n",
        "  return img_activations\n",
        "\n",
        "def get_output_of_the_model(model, image, device):\n",
        "  '''\n",
        "  This function is just a \"forward\" call in \"eval\" mode, in order to have the predictions for a given image.\n",
        "  '''\n",
        "  # put the model in the evaluation mode\n",
        "  model.eval()\n",
        "  # move input tensor x to the selected device\n",
        "  image = image.to(device)\n",
        "  # execute the model with gradients\n",
        "  output = model(image)\n",
        "  return(output)\n",
        "\n",
        "def get_activ(layer, target_layer, activ):\n",
        "  '''\n",
        "  This function will recursively search for the layer to be used in gradcam and compute the mean gradient of the output channels.\n",
        "  '''\n",
        "  if target_layer == layer:\n",
        "    grad = layer.weight.grad\n",
        "\n",
        "    for i in range(activ.shape[1]):\n",
        "      activ[:,i,:,:] *= grad[i].mean()\n",
        "      return activ\n",
        "  else:\n",
        "    # check another layer\n",
        "    if hasattr(layer, '__getitem__'): #__iter__\n",
        "      for in_layer in layer:\n",
        "        activ = get_activ(in_layer,target_layer,activ)\n",
        "        if activ != None:\n",
        "          return activ\n",
        "\n",
        "  return None\n",
        "\n",
        "def get_heatmap(model, image, target_layer, device):\n",
        "  '''\n",
        "  Find a heatmap for a given image and target_layer of the model.\n",
        "  \"device\" is the device where the model is running (to use GPU when available)\n",
        "  '''\n",
        "  image_in = image.unsqueeze(0)\n",
        "\n",
        "  # get the output of the feature extractor\n",
        "  activ  = get_activations(model, image_in, device)\n",
        "\n",
        "  # get the predictions at the output of the decision layer\n",
        "  logits = get_output_of_the_model(model, image_in, device)\n",
        "\n",
        "  # get the most confident prediction\n",
        "  pred   = logits.max(-1)[-1]\n",
        "\n",
        "  model.zero_grad()\n",
        "  logits[0,pred].backward(retain_graph=True)\n",
        "\n",
        "  for layer in model.features.children():\n",
        "    res = get_activ(layer, target_layer, activ)\n",
        "    if res != None:\n",
        "      break\n",
        "\n",
        "  if res == None:\n",
        "    raise Exception(\"Layer not found!\")\n",
        "\n",
        "  heatmap = torch.mean(res, dim=1)[0].cpu().detach()\n",
        "\n",
        "  heatmap = heatmap.squeeze(0).numpy()\n",
        "  # normalize image with minmax\n",
        "  heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "  # Resize figure to be in the same size as the input image\n",
        "  heatmap = cv2.resize(heatmap, (image.shape[2], image.shape[1]))\n",
        "\n",
        "  return (heatmap, labels_map[pred.cpu().detach().numpy()[0]])\n",
        "\n",
        "def display_image_with_heatmap(img, heatmap, scale, ax, true_label, predicted_label):\n",
        "  '''\n",
        "  Display the image with the heatmap in a given scale, with labels.\n",
        "  '''\n",
        "  heatmap = np.uint8(255.0*heatmap)\n",
        "  width   = int(heatmap.shape[1]*scale)\n",
        "  height  = int(heatmap.shape[0]*scale)\n",
        "  heatmap = cv2.resize(heatmap, (width, height))\n",
        "  img     = 255*(img - np.min(img))/(np.max(img)-np.min(img))\n",
        "  img     = cv2.resize(img, (width, height))\n",
        "  heatmap = cv2.applyColorMap(255-heatmap, cv2.COLORMAP_JET)\n",
        "  heatmap = np.uint8(heatmap)\n",
        "  heatmap = np.uint8(heatmap*0.3 + img*0.7)\n",
        "\n",
        "  ax.imshow(heatmap)\n",
        "  ax.set_title(\"True: \" + true_label + \" || Predicted:\" + predicted_label,{'fontsize':25})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BNQ7nJkl172P",
      "metadata": {
        "id": "BNQ7nJkl172P"
      },
      "outputs": [],
      "source": [
        "# Example of code to use the gradcam functions\n",
        "\n",
        "# Set multiple axes to plot multiple images at the same time\n",
        "fig, axes = plt.subplots(3, 3, figsize=(50,50), subplot_kw={'xticks':[], 'yticks':[]})\n",
        "\n",
        "# This can be done with any pytorch dataset, as example the \"trainset\" was chosed\n",
        "# \"preparation\" is representing the composed transformations that are used for validation/test (with no augmentation)\n",
        "# You should change this to your dataset class and test transformations.\n",
        "train_dataset = ImageDataset(trainset, preparation)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  image, true_label = train_dataset[i*10]\n",
        "\n",
        "  heatmap, predicted_label = get_heatmap(model, image, model.features[-1][0], device)\n",
        "  image = image.permute(1,2,0).numpy()\n",
        "\n",
        "  display_image_with_heatmap(image, heatmap, 4, ax, labels_map[true_label], predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fuGpkzWP40",
      "metadata": {
        "id": "22fuGpkzWP40"
      },
      "source": [
        "## Deadline\n",
        "\n",
        "Saturday, June 01, 11:59 pm.\n",
        "\n",
        "Penalty policy for late submission: You are not encouraged to submit your assignment after due date. However, in case you do, your grade will be penalized as follows:\n",
        "- June 02, 11:59 pm : grade * 0.75\n",
        "- June 03, 11:59 pm : grade * 0.5\n",
        "- June 04, 11:59 pm : grade * 0.25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PVHX7B46WSjI",
      "metadata": {
        "id": "PVHX7B46WSjI"
      },
      "source": [
        "## Submission\n",
        "\n",
        "On Google Classroom, submit your Jupyter Notebook (in Portuguese or English) or Google Colaboratory link (remember to share it!).\n",
        "\n",
        "**This activity is NOT individual, it must be done in pairs (two-person group).**\n",
        "\n",
        "Only one individual should deliver the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
