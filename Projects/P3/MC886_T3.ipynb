{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf6361a3-45da-4505-8e24-2fd53afd4bee",
      "metadata": {
        "id": "cf6361a3-45da-4505-8e24-2fd53afd4bee",
        "tags": []
      },
      "source": [
        "# Group information\n",
        "\n",
        "Names: Andreas Cisi Ramos e João Pedro de Moraes Novaes\n",
        "\n",
        "\n",
        "RAs: 246932 e 174494"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed070eb-2db6-4905-9f49-4a325bfde24d",
      "metadata": {
        "id": "3ed070eb-2db6-4905-9f49-4a325bfde24d",
        "tags": []
      },
      "source": [
        "## Objective:\n",
        "\n",
        "To explore **deep learning** techniques, focused on **Convolutional Neural Networks**. In this task you'll be architecturing different CNNs to solve an image classification problem.\n",
        "\n",
        "This **MUST** be developed using the pytorch and Sklearn libraries (PyTorch Lightning is **not** allowed).\n",
        "\n",
        "*Tip: Use the Pillow (PIL) library to work with images with pytorch. Also, you can use [tqdm](https://github.com/tqdm/tqdm) library to see the progress of the training process.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDX4xG1wKwH9",
      "metadata": {
        "id": "tDX4xG1wKwH9"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset you should use is an adaptation of the \"CIFAR10\" dataset. The CIFAR10 dataset is a dataset of 32x32 images of 10 different classes, and is extensivelly used for classification of images using CNN.\n",
        "\n",
        "In this work, You will only use CIFAR10 data of 3 classes: airplane, bird and cat. You also will only use a limited amount of the data of each class (the original dataset has 6000 images per class). For each class, the amount of images is 400 for training, 200 for validation and 300 for test. This is defined so that the training steps are faster, but it makes the problem harder, as we are working with few data points.\n",
        "\n",
        "\n",
        "The dataset will be available in the [same folder](https://drive.google.com/drive/folders/14uiy_7xMq5LOqODBzbIJLD4Vq0E9XD5v) as the other tasks, in the \"cifar_mod\" folder. You can copy the dataset folder or download it locally.\n",
        "\n",
        "Here are some examples of each class from the dataset:\n",
        "\n",
        "**airplane**\n",
        "\n",
        "**bird**\n",
        "\n",
        "\n",
        "**cat**\n",
        "\n",
        "\n",
        "\n",
        "As in every machine learning task, we need to understand and analyze the data. From the way that the dataset was collected (data collection protocol, equipment used, people involved), to its final result (resulting files) and objective. When dealing with images, we usually want to track possible biases that different classes may have when collected. This dataset contains images of the target objects in a centered position in the foreground, having few biases related to the common background of some classes (for example: airplanes in the blue sky).\n",
        "\n",
        "As in the other tasks we already covered the data analysis part, we will focus on the technical machine learning parts for this one. Just remember that images are another type of data and we also could (and **should**) analyze the information before applying machine learning techniques blindly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e9ef69-28e6-444f-8a04-e03f1ff34fbc",
      "metadata": {
        "id": "77e9ef69-28e6-444f-8a04-e03f1ff34fbc"
      },
      "source": [
        "## Load the dataset (2 Points)\n",
        "\n",
        "When working with Pytorch, we need to create a \"Dataset\" class that usually will handle the data loading, as well as the data transformations, and will allow us to retrieve the data with the respective label.\n",
        "\n",
        "Data transformations are an important part of Pytorch datasets, as we already studied in the lectures that data augmentation can be powerful in training deep neural networks. But some of those transformations are just to define the entry of the model as Pytorch tensors or normalizing data, which can be impactful as well.\n",
        "\n",
        "Create a Pytorch Dataset and Dataloader with and without data augmentation for training data (to compare later). The `Dataset` class has 3 required methods: `__init__`, `__len__`, and `__getitem__`. In the code below we have some basic idea of an ImageDataset.\n",
        "\n",
        "You can choose how to implement the image loading in the class:\n",
        "- Inside the `__getitem__` method (low memory usage for each dataset instance, slower for training)\n",
        "- Inside the `__init__` method (saves all images on memory, but training is faster)\n",
        "\n",
        "Either implementation is ok, if the machine you're using supports the loading of all images this is probably better for performance.\n",
        "\n",
        "For a deeper look at Pytorch datasets and dataloaders, look [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "The images are already divided by folder for train, validation and test, and you should follow this order. The targets of each image are described in the filename in the following pattern: `[image_number]_[class].jpg`.\n",
        "\n",
        "To create the data augmentation you can follow the `preparation` example below, and add different Pytorch transformations. A list of available transformations (as well as examples) can be found in the [official documentation (here)](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "IbEvl-1GHkNT",
      "metadata": {
        "id": "IbEvl-1GHkNT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# IMPORTS\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "OZC2Q6qmvi13",
      "metadata": {
        "id": "OZC2Q6qmvi13"
      },
      "outputs": [],
      "source": [
        "input_width =  32 \n",
        "input_height = 32\n",
        "nchannels = 3\n",
        "\n",
        "# train transformations - Data Augmentation : transformaçoes como flip / rotação / brilho / zoom / crop\n",
        "augmentation = transforms.Compose([\n",
        "    transforms.Resize((input_width, input_height), interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# test transformations - here we should only have the transformations needed for preparing the data for the model\n",
        "preparation = transforms.Compose([\n",
        "              # make all images the same size with a specific interpolation method\n",
        "              transforms.Resize((input_width,input_height), interpolation=transforms.InterpolationMode.BILINEAR,\n",
        "                                max_size=None, antialias=True),\n",
        "              # transform the image to Tensor (this will change the configuration from Height x Width x Channels to Channels x Height x Width)\n",
        "              transforms.ToTensor(),\n",
        "              # this normalization is just an example, based on the ImageNet mean and standard deviation of images. You can test without it if you want!\n",
        "              transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "          ])\n",
        "\n",
        "class ImageDataset():\n",
        "  def __init__(self, directory, transform=None):\n",
        "    self.directory = directory\n",
        "    self.transform = transform\n",
        "    self.filenames = []\n",
        "    self.class_to_idx = {0:'airplane', 1:'bird', 2:'cat'}\n",
        "    \n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                self.filenames.append(os.path.join(root, file))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.filenames) # Attention: change this line if your __init__ method uses another attribute to keep the images/filenames\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    Return a tuple with the image (as a Tensor) and the respective target at position idx.\n",
        "    '''\n",
        "    image_path = self.filenames[idx]\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    print(\"CHEGUEI 1\")\n",
        "    # Adjusting label extraction logic\n",
        "    label_str = os.path.basename(image_path).split('_')[-1].split('.')[0]\n",
        "    print(label_str)\n",
        "    # Try extracting the class name\n",
        "    \n",
        "    \n",
        "    label = self.class_to_idx[ label_str]  # Default to -1 if label not found\n",
        "    \n",
        "    if label == -1:\n",
        "        raise ValueError(f\"Label '{label_str}' not found in class_to_idx mapping.\")\n",
        "    \n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    \n",
        "    return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "_x1yZOzLAjJG",
      "metadata": {
        "id": "_x1yZOzLAjJG"
      },
      "outputs": [],
      "source": [
        "# Definir os diretórios do dataset\n",
        "train_dir = 'cifar_mod/train'\n",
        "val_dir = 'cifar_mod/val'\n",
        "test_dir = 'cifar_mod/test'\n",
        "\n",
        "# Criar os datasets\n",
        "train_dataset_augmented = ImageDataset(directory=train_dir, transform=augmentation)\n",
        "val_dataset = ImageDataset(directory=val_dir, transform=preparation)\n",
        "test_dataset = ImageDataset(directory=test_dir, transform=preparation)\n",
        "\n",
        "# Criar os dataloaders\n",
        "# batch_size = 32\n",
        "\n",
        "# train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ed36db73-2950-4360-8b9f-6de062bf3991",
      "metadata": {
        "id": "ed36db73-2950-4360-8b9f-6de062bf3991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 1200\n",
            "CHEGUEI 1\n",
            "jpg\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Label 'jpg' not found in class_to_idx mapping.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m datatensor \u001b[38;5;241m=\u001b[39m train_dataset_augmented\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(datatensor))\n\u001b[1;32m----> 5\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdatatensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# This will execute the '__getitem__' method\u001b[39;00m\n\u001b[0;32m      6\u001b[0m nchannels \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m height    \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
            "Cell \u001b[1;32mIn[30], line 60\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx\u001b[38;5;241m.\u001b[39mget(label_str, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Default to -1 if label not found\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in class_to_idx mapping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     63\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
            "\u001b[1;31mValueError\u001b[0m: Label 'jpg' not found in class_to_idx mapping."
          ]
        }
      ],
      "source": [
        "# Visualize images from the training set\n",
        "\n",
        "datatensor = train_dataset_augmented\n",
        "print(\"Number of images:\", len(datatensor))\n",
        "image, target = datatensor[100] # This will execute the '__getitem__' method\n",
        "nchannels = image.shape[0]\n",
        "height    = image.shape[1]\n",
        "width     = image.shape[2]\n",
        "image     = image.permute(1,2,0).numpy() # Converts the Tensor back to image shape\n",
        "image     = 255*(image - np.min(image))/(np.max(image)-np.min(image))\n",
        "image     = image.astype('uint8')\n",
        "print(\"Images are {}x{}x{}\".format(width,height,nchannels))\n",
        "plt.imshow(image)\n",
        "print(\"Class of the image: \", target)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6rzFdKIWOvt",
      "metadata": {
        "id": "H6rzFdKIWOvt"
      },
      "source": [
        "## Train a baseline model (1 Point)\n",
        "\n",
        "Before going for the deep learning approach, you should test a baseline model in this problem. To do so, train a RandomForectClassifier, where the inputs are the flattened images (all pixes of 3 channels concatenated).\n",
        "\n",
        "You should use the same train/validation/test division that you'll be using in the next section. You can use the Sklearn library for this task. Remember to test and plot a confusion matrix with the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pp9lA-2WS4-",
      "metadata": {
        "id": "8pp9lA-2WS4-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ccc6366c-d7f2-4a9b-8c8e-08cfad39f3b7",
      "metadata": {
        "id": "ccc6366c-d7f2-4a9b-8c8e-08cfad39f3b7"
      },
      "source": [
        "## Build a CNN (6 Points)\n",
        "\n",
        "In this section, you should construct and validate a CNN architecture for the given problem.\n",
        "\n",
        "It's required that you test at least the following parts of a CNN:\n",
        "- Number of convolutional layers and different output sizes\n",
        "- Kernel size of filters\n",
        "- Number of training epochs\n",
        "- Learning rate of the network\n",
        "- Batchsize used in dataloader\n",
        "- Stride on convolutional or pooling layers\n",
        "- Different architecture for the final dense layer (with or without hidden layers and their size)\n",
        "\n",
        "Also, you must compare at least for one network if the data augmentation is helping the training process or not.\n",
        "\n",
        "Lastly, you must change the `Criterion` function to return the balanced accuracy instead of the normal one.\n",
        "\n",
        "--------\n",
        "\n",
        "The process to build a CNN is an exploratory analysis and should be done carefully. As this is a costly process, you will not test all possibilities between each other, you should interpret the results at each step and understand what is happening or not happening.\n",
        "\n",
        "As this is a costly process, you should use the normal division of train/validation/test, instead of cross validation. Also, at each network built and trained/validated, you should look at the results to understand if any overfitting or underfitting is happening. Save the values of each batch of train and validation to analyze how the model performed over the epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eL43xkO2e41S",
      "metadata": {
        "id": "eL43xkO2e41S"
      },
      "source": [
        "### Exploding/vanishing gradients\n",
        "\n",
        "As we already learned, the weights of a neural network are updated using the backpropagation algorithm. As our network grows in depth, there are two problems that can occur with the backpropagation algorithm, known as Exploding and Vanishing gradients.\n",
        "\n",
        "As both names imply, both conditions relate to a gradient calculation that goes out of control for some reason.\n",
        "\n",
        "The exploding gradient can happen when the current weights generate a very large loss, and all the network weights are updated by a huge amount. This can lead to a cycle where the network is not able to learn because every learning step impacts too much the network, making it very unstable.\n",
        "\n",
        "On the other side, the vanishing gradient will happen when the gradients get too small, and the updates are not able to propagate to the initial layers.\n",
        "\n",
        "There are a couple of ways to solve those issues. The first is to use an activation function that is \"non-saturating\". The sigmoid function is an example of a saturating function because its derivative tends to 0 on larger positive or negative values. The best activation functions to avoid this issue are the ReLU and its variations (LReLU, PReLU, ELU, etc). All those examples help to mitigate the vanishing gradient.\n",
        "\n",
        "Another important step is to initialize the weights of the network properly. Initializing the weights randomly can also lead to vanishing/exploding gradients in some situations. The common strategy is to use a heuristic. We are not going to cover this manner in a deeper aspect, but the Xavier initialization is already implemented in the example below.\n",
        "\n",
        "Finally, the best thing to avoid vanishing and exploding gradients in a deep CNN is to use the Batch Normalization technique. This technique is a new operation on the CNN, that normalizes its input and adds a parameter for scale and another for shifting. This enables the network to learn the optimal scale and mean of the layer. This normalization uses the data of the batch of images being executed on the model to find the mean and standard deviation. In pytorch, there is a function to use this technique in the `nn` module: `nn.BatchNorm2d()`. The BatchNorm operation can be used before or after the activation function.\n",
        "\n",
        "For example:\n",
        "- Conv -> BatchNorm -> Activation(ReLU) -> Pooling\n",
        "Or\n",
        "- Conv -> Activation(ReLU) -> BatchNorm -> Pooling).\n",
        "\n",
        "In this task, you may face problems concerning the stability and convergence of the network because of the exploding or vanishing gradients, and it is recommended that you use the techniques cited to avoid this issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ru9eRQukb8mJ",
      "metadata": {
        "id": "Ru9eRQukb8mJ"
      },
      "outputs": [],
      "source": [
        "gpu = torch.cuda.is_available()\n",
        "\n",
        "if not gpu:\n",
        "  print(\"GPU not available!\")\n",
        "\n",
        "device = torch.device(0) if gpu else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BD2bBcJ6eCzU",
      "metadata": {
        "id": "BD2bBcJ6eCzU"
      },
      "source": [
        "**Saving models**\n",
        "\n",
        "As the CNN training process can take some time, is interesting to save your models, to avoid re-training in the case of a crash or just to save your work for other time.\n",
        "\n",
        "The next cell shows a simple code to save and load pytorch models. Remember to keep the class of the model in the same way, as the state_dict will not work if you change the architecture. Create new classes for different tests, with names that correctly describe what is being tested.\n",
        "\n",
        "**Note**: If you're using google colab, just saving the model is not enough. You need to save it for a folder in your google Drive, or download the files after saving them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ywu0txFRa8C0",
      "metadata": {
        "id": "Ywu0txFRa8C0"
      },
      "outputs": [],
      "source": [
        "# ===== How to save a pytorch model =====\n",
        "\n",
        "## Since you have the class of the model, you can save the weights\n",
        "## IMPORTANT: If you change the class implementation you will NOT be able to load the model again.\n",
        "torch.save(model.to('cpu').state_dict(), \"/path/to/save\")\n",
        "\n",
        "# ===== How to load a pytorch model =====\n",
        "\n",
        "## Instantiate the model class as usual\n",
        "model = Model(input_shape, nclasses)\n",
        "\n",
        "## Load the state dict saved before\n",
        "state_dict  = torch.load(\"/path/to/save\")\n",
        "\n",
        "## Load the weights in the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "## Send the model to the GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E832WhmRV8Fw",
      "metadata": {
        "id": "E832WhmRV8Fw"
      },
      "source": [
        "**Example of train/validate code**\n",
        "\n",
        "In the next cells, we show an example of how to develop the train/validation functions using pytorch. You don't need to use this code, but it is a place to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i-_hHXzcV70s",
      "metadata": {
        "id": "i-_hHXzcV70s"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_batch(model, data, optimizer, criterion, device):\n",
        "  '''\n",
        "  Funtion that trains a batch of data in the 'device' (that should be a GPU)\n",
        "  'data' is a batch of the dataloader.\n",
        "  'optmizer' should be an instance of an pytorch optmizer. You don't need to test different optmizers, just go with SGD (torch.optim.SGD).\n",
        "  'criterion' is a custom function to calculate the loss and accuracy of the predictions of the batch.\n",
        "  'device' is the device that the model is loaded on.\n",
        "  '''\n",
        "  model.train()\n",
        "  ims, targets = data\n",
        "  ims = ims.to(device=device)\n",
        "  targets = targets.to(device=device)\n",
        "  preds = model(ims)\n",
        "  loss, acc = criterion(model, preds, targets, device)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  return loss.item(), acc.item()\n",
        "\n",
        "# This line avoid that the model weights change during validation\n",
        "@torch.no_grad()\n",
        "def validate_batch(model, data, criterion, device):\n",
        "  '''\n",
        "  Funtion that validates a batch of data in the 'device' (that should be a GPU).\n",
        "  Same parameters as 'train_batch', but this time no optmizer is needed.\n",
        "  '''\n",
        "  model.eval()\n",
        "  ims, targets = data\n",
        "  ims = ims.to(device=device)\n",
        "  targets = targets.to(device=device)\n",
        "  preds = model(ims)\n",
        "  loss, acc = criterion(model, preds, targets, device)\n",
        "\n",
        "  return loss.item(), acc.item()\n",
        "\n",
        "def Criterion(model, preds, targets, device):\n",
        "  '''\n",
        "  Function that calculates the loss and accuracy of a batch predicted by the model.\n",
        "  '''\n",
        "  ce            = nn.CrossEntropyLoss().to(device) # You don't need to change the loss function (but you can if it makes sense on your analysis)\n",
        "  loss          = ce(preds, targets.long())\n",
        "  pred_labels   = torch.max(preds.data, 1)[1] # same as argmax\n",
        "  acc           = torch.sum(pred_labels == targets.data)\n",
        "  n             = pred_labels.size(0)\n",
        "  acc           = acc/n\n",
        "\n",
        "  return loss, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyecR67TZ2YT",
      "metadata": {
        "id": "kyecR67TZ2YT"
      },
      "outputs": [],
      "source": [
        "class ExampleCNN(nn.Module):\n",
        "  def __init__(self, input_shape, num_classes):\n",
        "    super(ExampleCNN, self).__init__()\n",
        "\n",
        "    # add network layers!\n",
        "\n",
        "    # lastly, initialize the weights\n",
        "    self._initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Executes a forward pass in the network with the input 'x'.\n",
        "    'x' can be a batch or a single image in the expected input_shape.\n",
        "    '''\n",
        "    # TODO: return prediction of the network\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    '''\n",
        "    Initialize the network weights using the Xavier initialization.\n",
        "    '''\n",
        "    for x in self.modules():\n",
        "      if isinstance(x, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(x.weight.data)\n",
        "        if (x.bias is not None):\n",
        "          x.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IYFtXlG-bNu7",
      "metadata": {
        "id": "IYFtXlG-bNu7"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "# Using Stochastic Gradient Descent\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_LmrG2YKV3B5",
      "metadata": {
        "id": "_LmrG2YKV3B5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0077b9bd-3316-4554-9f98-9cfcab7a9177",
      "metadata": {
        "id": "0077b9bd-3316-4554-9f98-9cfcab7a9177"
      },
      "source": [
        "### Early stoping regularization (EXTRA: 1 Point)\n",
        "\n",
        "This can only be done if you are able to find a CNN that was able to overfit the train dataset.\n",
        "\n",
        "If that is the case, change the training function in order to perform an [early stopping](https://en.wikipedia.org/wiki/Early_stopping). The early stopping technique using validation is a technique to stop the training process when a defined condition is achieved. Find this condition and change the code.\n",
        "\n",
        "Discuss the results and why they happened (it works? Why?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf14ea2-42e3-46bd-be64-b1c38d7e83da",
      "metadata": {
        "id": "3cf14ea2-42e3-46bd-be64-b1c38d7e83da"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9d0bcc1a-a089-456c-9ea0-d79e3c8f54df",
      "metadata": {
        "id": "9d0bcc1a-a089-456c-9ea0-d79e3c8f54df"
      },
      "source": [
        "## Interpertability on CNNs (1 point)\n",
        "\n",
        "The Gradient-weighted Class Activation Mapping (Grad-CAM) technique uses the gradient of one or more convolutional layers to highlight the regions of the image that were more impactful to a prediction. For more details, you can read [this paper](https://arxiv.org/abs/1610.02391).\n",
        "\n",
        "The idea is to use the gradient to create a heatmap indicating the parts that had more weight on the decision for a given sample.\n",
        "\n",
        "In the next cells, the Grad-CAM code is defined for one layer. You should use this code to test a few images of your best CNN defined in the previous sections. You can change the visualizations if you want.\n",
        "\n",
        "Analyze the results of Grad-CAM for different images and classes, and discuss whether the model is focusing or not on the right parts of the image and why this may be happening. Feel free to change the code below to visualize in a different way (different quantity of images, different labeling, etc).\n",
        "\n",
        "-------\n",
        "\n",
        "**Important: The code below has some assumptions about the network architecture. If the model class is built in a different way you MUST change the implementation of the `get_activations` function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb5b97d-9554-4515-9970-22308dd2e77f",
      "metadata": {
        "id": "5fb5b97d-9554-4515-9970-22308dd2e77f"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels_map = {\n",
        "  0: \"Airplane\",\n",
        "  1: \"Bird\",\n",
        "  2: \"Cat\",\n",
        "}\n",
        "\n",
        "def get_activations(model, image, device):\n",
        "  '''\n",
        "  This function return the activations for a given image.\n",
        "  The assumption is that there is a \"features\" attribute that contains the convolutional part of the network.\n",
        "  If this is not the case of your network, change the code below to run the network until the last convolutional layer (with activations), and return it.\n",
        "  \"image\" is the selected image to get activations\n",
        "  \"device\" is the device where the model is running (to use the GPU when available)\n",
        "  '''\n",
        "  # move input tensor x to the selected device\n",
        "  image = image.to(device)\n",
        "  # get activations after feature extraction\n",
        "  img_activations = model.features(image)\n",
        "\n",
        "  return img_activations\n",
        "\n",
        "def get_output_of_the_model(model, image, device):\n",
        "  '''\n",
        "  This function is just a \"forward\" call in \"eval\" mode, in order to have the predictions for a given image.\n",
        "  '''\n",
        "  # put the model in the evaluation mode\n",
        "  model.eval()\n",
        "  # move input tensor x to the selected device\n",
        "  image = image.to(device)\n",
        "  # execute the model with gradients\n",
        "  output = model(image)\n",
        "  return(output)\n",
        "\n",
        "def get_activ(layer, target_layer, activ):\n",
        "  '''\n",
        "  This function will recursively search for the layer to be used in gradcam and compute the mean gradient of the output channels.\n",
        "  '''\n",
        "  if target_layer == layer:\n",
        "    grad = layer.weight.grad\n",
        "\n",
        "    for i in range(activ.shape[1]):\n",
        "      activ[:,i,:,:] *= grad[i].mean()\n",
        "      return activ\n",
        "  else:\n",
        "    # check another layer\n",
        "    if hasattr(layer, '__getitem__'): #__iter__\n",
        "      for in_layer in layer:\n",
        "        activ = get_activ(in_layer,target_layer,activ)\n",
        "        if activ != None:\n",
        "          return activ\n",
        "\n",
        "  return None\n",
        "\n",
        "def get_heatmap(model, image, target_layer, device):\n",
        "  '''\n",
        "  Find a heatmap for a given image and target_layer of the model.\n",
        "  \"device\" is the device where the model is running (to use GPU when available)\n",
        "  '''\n",
        "  image_in = image.unsqueeze(0)\n",
        "\n",
        "  # get the output of the feature extractor\n",
        "  activ  = get_activations(model, image_in, device)\n",
        "\n",
        "  # get the predictions at the output of the decision layer\n",
        "  logits = get_output_of_the_model(model, image_in, device)\n",
        "\n",
        "  # get the most confident prediction\n",
        "  pred   = logits.max(-1)[-1]\n",
        "\n",
        "  model.zero_grad()\n",
        "  logits[0,pred].backward(retain_graph=True)\n",
        "\n",
        "  for layer in model.features.children():\n",
        "    res = get_activ(layer, target_layer, activ)\n",
        "    if res != None:\n",
        "      break\n",
        "\n",
        "  if res == None:\n",
        "    raise Exception(\"Layer not found!\")\n",
        "\n",
        "  heatmap = torch.mean(res, dim=1)[0].cpu().detach()\n",
        "\n",
        "  heatmap = heatmap.squeeze(0).numpy()\n",
        "  # normalize image with minmax\n",
        "  heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "  # Resize figure to be in the same size as the input image\n",
        "  heatmap = cv2.resize(heatmap, (image.shape[2], image.shape[1]))\n",
        "\n",
        "  return (heatmap, labels_map[pred.cpu().detach().numpy()[0]])\n",
        "\n",
        "def display_image_with_heatmap(img, heatmap, scale, ax, true_label, predicted_label):\n",
        "  '''\n",
        "  Display the image with the heatmap in a given scale, with labels.\n",
        "  '''\n",
        "  heatmap = np.uint8(255.0*heatmap)\n",
        "  width   = int(heatmap.shape[1]*scale)\n",
        "  height  = int(heatmap.shape[0]*scale)\n",
        "  heatmap = cv2.resize(heatmap, (width, height))\n",
        "  img     = 255*(img - np.min(img))/(np.max(img)-np.min(img))\n",
        "  img     = cv2.resize(img, (width, height))\n",
        "  heatmap = cv2.applyColorMap(255-heatmap, cv2.COLORMAP_JET)\n",
        "  heatmap = np.uint8(heatmap)\n",
        "  heatmap = np.uint8(heatmap*0.3 + img*0.7)\n",
        "\n",
        "  ax.imshow(heatmap)\n",
        "  ax.set_title(\"True: \" + true_label + \" || Predicted:\" + predicted_label,{'fontsize':25})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BNQ7nJkl172P",
      "metadata": {
        "id": "BNQ7nJkl172P"
      },
      "outputs": [],
      "source": [
        "# Example of code to use the gradcam functions\n",
        "\n",
        "# Set multiple axes to plot multiple images at the same time\n",
        "fig, axes = plt.subplots(3, 3, figsize=(50,50), subplot_kw={'xticks':[], 'yticks':[]})\n",
        "\n",
        "# This can be done with any pytorch dataset, as example the \"trainset\" was chosed\n",
        "# \"preparation\" is representing the composed transformations that are used for validation/test (with no augmentation)\n",
        "# You should change this to your dataset class and test transformations.\n",
        "train_dataset = ImageDataset(trainset, preparation)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  image, true_label = train_dataset[i*10]\n",
        "\n",
        "  heatmap, predicted_label = get_heatmap(model, image, model.features[-1][0], device)\n",
        "  image = image.permute(1,2,0).numpy()\n",
        "\n",
        "  display_image_with_heatmap(image, heatmap, 4, ax, labels_map[true_label], predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742937dc-1df5-442d-b7ca-74108dbfbe88",
      "metadata": {
        "id": "742937dc-1df5-442d-b7ca-74108dbfbe88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vh4LcitLR0Vw",
      "metadata": {
        "id": "Vh4LcitLR0Vw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmA3-6lKWPmm",
      "metadata": {
        "id": "wmA3-6lKWPmm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "22fuGpkzWP40",
      "metadata": {
        "id": "22fuGpkzWP40"
      },
      "source": [
        "## Deadline\n",
        "\n",
        "Saturday, June 01, 11:59 pm.\n",
        "\n",
        "Penalty policy for late submission: You are not encouraged to submit your assignment after due date. However, in case you do, your grade will be penalized as follows:\n",
        "- June 02, 11:59 pm : grade * 0.75\n",
        "- June 03, 11:59 pm : grade * 0.5\n",
        "- June 04, 11:59 pm : grade * 0.25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PVHX7B46WSjI",
      "metadata": {
        "id": "PVHX7B46WSjI"
      },
      "source": [
        "## Submission\n",
        "\n",
        "On Google Classroom, submit your Jupyter Notebook (in Portuguese or English) or Google Colaboratory link (remember to share it!).\n",
        "\n",
        "**This activity is NOT individual, it must be done in pairs (two-person group).**\n",
        "\n",
        "Only one individual should deliver the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
